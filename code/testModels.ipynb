{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AttentivePooling(nn.Module):\n",
    "    def __init__(self, dim1: int, dim2: int):\n",
    "        super(AttentivePooling, self).__init__()\n",
    "        self.dim1 = dim1\n",
    "        self.dim2 = dim2\n",
    "\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.dense  = nn.Linear(dim2, 200)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.flatten = nn.Linear(200, 1)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "       \n",
    "\n",
    "    def forward(self, x):\n",
    "        user_vecs = self.dropout(x)\n",
    "        user_att = self.tanh(self.dense(user_vecs))\n",
    "        user_att = self.flatten(user_att)\n",
    "        user_att = self.softmax(user_att)\n",
    "        result = torch.einsum('ijk,ijk->ik', user_vecs, user_att)\n",
    "        return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim1 = 5\n",
    "dim2 = 10\n",
    "x = torch.randn((2, dim1, dim2))\n",
    "\n",
    "pool = AttentivePooling(dim1, dim2)\n",
    "\n",
    "pool(x).shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "def tfAttentivePooling(dim1,dim2):\n",
    "    vecs_input = Input(shape=(dim1,dim2),dtype='float32')\n",
    "    user_vecs =Dropout(0.2)(vecs_input)\n",
    "    user_att = Dense(200,activation='tanh')(user_vecs)\n",
    "    user_att = keras.layers.Flatten()(Dense(1)(user_att))\n",
    "    user_att = Activation('softmax')(user_att)    \n",
    "    user_vec = keras.layers.Dot((1,1))([user_vecs,user_att])\n",
    "    model = Model(vecs_input,user_vec)\n",
    "    return model\n",
    "\n",
    "def testDot(dim1, dim2):\n",
    "    input = Input(shape=(dim1,dim2), dtype='float32')\n",
    "    output = keras.layers.Dot((2,2))([input, input])\n",
    "    return Model(input, output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-12 02:54:20.549877: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
      "2022-05-12 02:54:20.549931: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-05-12 02:54:20.549937: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
      "2022-05-12 02:54:20.549941: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
      "2022-05-12 02:54:20.550022: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3314 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0001:00:00.0, compute capability: 7.5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[14, 32],\n",
       "        [32, 77]]], dtype=int32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = testDot(2,3)\n",
    "x_tf = tf.convert_to_tensor([[[1,2,3],[4,5,6]]])\n",
    "t(x_tf).eval(session=tf.compat.v1.Session())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_16 (None, 30, 300)\n",
      "dropout_15 (None, 30, 300)\n",
      "dense_19 (None, 30, 200)\n",
      "dense_20 (None, 30, 1)\n",
      "flatten_10 (None, 30)\n",
      "activation_11 (None, 30)\n",
      "dot_14 (None, 300)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'model_15/dot_14/Squeeze:0' shape=(2, 300) dtype=float32>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool = tfAttentivePooling(dim1, dim2)\n",
    "\n",
    "def print_model(m):\n",
    "    for layer in m.layers:\n",
    "        print(layer.name, layer.output_shape) \n",
    "\n",
    "print_model(pool)\n",
    "x_tf = tf.convert_to_tensor(x.numpy())\n",
    "\n",
    "pool(x_tf) #.eval(session=tf.compat.v1.Session())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_11 (None, 30, 300)\n",
      "dropout_8 (None, 30, 300)\n",
      "conv1d_2 (None, 28, 400)\n",
      "dropout_9 (None, 28, 400)\n",
      "attention_1 (None, 28, 400)\n",
      "activation_6 (None, 28, 400)\n",
      "dropout_10 (None, 28, 400)\n",
      "model_10 (None, 400)\n"
     ]
    }
   ],
   "source": [
    "from models import Attention\n",
    "\n",
    "def get_doc_encoder():\n",
    "    sentence_input = Input(shape=(30,300), dtype='float32')\n",
    "    droped_vecs = Dropout(0.2)(sentence_input)\n",
    "\n",
    "    l_cnnt = Conv1D(400,3,activation='relu')(droped_vecs)\n",
    "    l_cnnt = Dropout(0.2)(l_cnnt)\n",
    "    l_cnnt = Attention(20,20)([l_cnnt,l_cnnt,l_cnnt])\n",
    "    l_cnnt = keras.layers.Activation('relu')(l_cnnt)\n",
    "    \n",
    "    droped_rep = Dropout(0.2)(l_cnnt)\n",
    "    title_vec = AttentivePooling(30,400)(droped_rep)\n",
    "    sentEncodert = Model(sentence_input, title_vec)\n",
    "    return sentEncodert\n",
    "\n",
    "d = get_doc_encoder()\n",
    "print_model(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Permute(nn.Module):\n",
    "    def __init__(self, *dims):\n",
    "        super(Permute, self).__init__()\n",
    "        self.dims = dims\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x.permute(*self.dims)\n",
    "\n",
    "class DocEncoder(nn.Module):\n",
    "    def __init__(self):        \n",
    "        super(DocEncoder,self).__init__()\n",
    "        self.phase1 = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            Permute(0,2,1),            \n",
    "            nn.Conv1d(300,400,3),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            Permute(0,2,1)\n",
    "        )\n",
    "        self.attention = nn.MultiheadAttention(400,20)\n",
    "        self.phase2 = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            AttentivePooling(30,400)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        l_cnnt = self.phase1(x)\n",
    "        print(l_cnnt.shape)\n",
    "        l_cnnt, attention_weights = self.attention(l_cnnt, l_cnnt, l_cnnt)\n",
    "        print(l_cnnt.shape)\n",
    "        result = self.phase2(l_cnnt)\n",
    "        print(result.shape)\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 28, 400])\n",
      "torch.Size([2, 28, 400])\n",
      "torch.Size([2, 400])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[2.3963e-03, 4.0591e-02, 0.0000e+00, 8.7075e-02, 1.4391e-02, 3.6700e-02,\n",
       "         3.3882e-02, 1.2511e-01, 5.3376e-03, 9.0946e-03, 6.6884e-02, 2.0321e-03,\n",
       "         0.0000e+00, 1.3617e-01, 2.1996e-02, 2.0637e-01, 1.9337e-01, 6.6152e-02,\n",
       "         7.2891e-02, 7.8411e-02, 1.2602e-01, 2.0873e-02, 9.7937e-03, 1.0884e-01,\n",
       "         0.0000e+00, 4.0968e-02, 2.3909e-02, 1.7039e-01, 6.3651e-02, 1.2156e-01,\n",
       "         3.3234e-02, 2.9967e-02, 1.2508e-02, 8.0818e-02, 4.1254e-02, 1.1412e-01,\n",
       "         2.1410e-02, 1.3321e-01, 1.5358e-02, 1.9235e-01, 5.7579e-04, 1.0120e-01,\n",
       "         7.8862e-02, 6.2540e-02, 1.6081e-02, 3.7133e-02, 1.9012e-01, 5.8300e-02,\n",
       "         2.1432e-01, 1.7429e-02, 1.4498e-01, 2.8760e-01, 0.0000e+00, 1.0378e-01,\n",
       "         7.4191e-02, 9.5585e-02, 1.1763e-03, 1.0734e-01, 1.1476e-01, 2.4840e-02,\n",
       "         5.0252e-02, 7.1241e-02, 1.2763e-01, 7.8959e-02, 2.2823e-02, 1.1548e-02,\n",
       "         6.8733e-02, 0.0000e+00, 2.0620e-01, 6.6682e-02, 6.1807e-04, 7.8888e-02,\n",
       "         1.3336e-02, 4.1974e-02, 7.2469e-02, 8.4554e-05, 1.3271e-01, 3.5570e-02,\n",
       "         1.5885e-01, 2.8756e-02, 8.0167e-02, 4.9624e-03, 1.1631e-02, 2.9368e-02,\n",
       "         2.5252e-02, 1.1689e-01, 7.3236e-02, 3.6508e-02, 0.0000e+00, 6.4463e-03,\n",
       "         6.1498e-02, 1.4280e-01, 1.8985e-02, 2.6849e-04, 5.3840e-03, 1.5818e-01,\n",
       "         1.0484e-01, 2.4588e-02, 1.5220e-01, 1.4661e-02, 1.8192e-01, 1.3577e-02,\n",
       "         1.8830e-01, 4.0313e-02, 6.9203e-03, 1.2492e-03, 3.8456e-02, 4.1987e-02,\n",
       "         6.1796e-02, 6.3017e-03, 5.0552e-02, 1.2279e-03, 1.0025e-02, 7.2828e-02,\n",
       "         2.4451e-02, 7.2650e-02, 2.7770e-02, 5.3932e-02, 6.2645e-02, 4.2171e-02,\n",
       "         3.3959e-02, 5.8230e-02, 7.0405e-02, 4.5663e-02, 8.6714e-02, 2.1461e-01,\n",
       "         7.8842e-02, 1.5573e-02, 6.3946e-02, 1.7922e-02, 1.9307e-02, 0.0000e+00,\n",
       "         1.0725e-01, 5.4317e-03, 1.7458e-01, 3.5563e-02, 0.0000e+00, 1.2929e-01,\n",
       "         5.0482e-02, 1.0993e-01, 2.0089e-02, 6.3472e-02, 6.5156e-03, 4.2996e-02,\n",
       "         1.9681e-01, 2.5746e-02, 6.1948e-02, 1.6553e-02, 6.3659e-02, 1.4406e-02,\n",
       "         1.5443e-03, 1.6999e-01, 9.4285e-03, 1.8731e-02, 0.0000e+00, 0.0000e+00,\n",
       "         8.9529e-03, 7.5267e-02, 1.1019e-01, 1.4273e-01, 1.5240e-01, 1.8824e-01,\n",
       "         2.4752e-01, 2.0007e-02, 1.4375e-03, 3.0795e-02, 4.0828e-02, 1.4599e-01,\n",
       "         2.6997e-01, 0.0000e+00, 1.4591e-02, 1.2836e-01, 3.3885e-02, 4.5431e-02,\n",
       "         9.5215e-02, 3.4027e-02, 2.8080e-02, 7.3433e-02, 6.6988e-02, 1.0456e-02,\n",
       "         1.2790e-01, 3.4968e-02, 2.0187e-02, 5.8312e-02, 1.9080e-01, 2.8767e-02,\n",
       "         1.7113e-01, 2.1894e-01, 3.1788e-03, 3.2381e-02, 6.5225e-03, 1.1856e-01,\n",
       "         0.0000e+00, 1.4275e-02, 6.6628e-03, 1.0712e-02, 1.7468e-01, 3.8243e-02,\n",
       "         5.9000e-02, 1.8002e-01, 7.8325e-03, 2.7802e-03, 1.0284e-01, 7.9121e-02,\n",
       "         5.3011e-02, 9.3600e-02, 6.5296e-03, 3.3604e-01, 0.0000e+00, 6.9448e-02,\n",
       "         1.3336e-01, 0.0000e+00, 1.1664e-01, 1.3398e-01, 4.4483e-02, 4.2162e-02,\n",
       "         5.0822e-02, 5.5810e-02, 1.8006e-01, 3.9210e-02, 1.8624e-02, 3.4886e-02,\n",
       "         1.9946e-02, 2.7885e-01, 0.0000e+00, 5.1063e-02, 4.3394e-03, 0.0000e+00,\n",
       "         4.2314e-02, 2.0540e-01, 1.5333e-01, 1.7125e-01, 1.7960e-02, 6.3498e-03,\n",
       "         2.4612e-02, 3.4163e-02, 1.2473e-01, 4.8592e-02, 1.6997e-01, 0.0000e+00,\n",
       "         2.0841e-02, 8.4356e-02, 2.5302e-02, 8.5360e-02, 8.5805e-03, 5.8769e-02,\n",
       "         1.7160e-01, 5.0144e-02, 1.5688e-01, 3.1084e-02, 4.3717e-02, 4.9069e-02,\n",
       "         2.9399e-03, 4.4020e-02, 1.6280e-02, 3.3832e-02, 8.7847e-02, 1.7565e-02,\n",
       "         1.7367e-01, 1.8630e-01, 1.3672e-02, 2.0511e-01, 6.5183e-01, 7.9784e-02,\n",
       "         4.0926e-02, 1.1886e-01, 5.4978e-03, 8.8635e-02, 1.0081e-01, 6.9846e-02,\n",
       "         4.2751e-02, 1.0088e-01, 2.2909e-02, 8.2307e-02, 3.2363e-02, 9.4175e-02,\n",
       "         5.9309e-02, 1.1612e-01, 1.3906e-01, 8.1894e-03, 6.3945e-02, 8.3749e-02,\n",
       "         0.0000e+00, 1.3756e-01, 1.2412e-02, 2.2879e-01, 8.2457e-02, 1.4542e-02,\n",
       "         0.0000e+00, 4.6342e-02, 3.3910e-01, 1.3782e-01, 2.9541e-02, 8.7147e-02,\n",
       "         3.3568e-02, 7.0589e-03, 5.2992e-02, 1.0842e-02, 1.8692e-01, 1.2089e-01,\n",
       "         4.3930e-02, 6.3750e-02, 2.9131e-01, 1.7956e-02, 2.9259e-02, 6.7314e-02,\n",
       "         7.1733e-02, 5.5569e-02, 5.7853e-02, 2.0594e-01, 4.9743e-03, 1.3193e-02,\n",
       "         9.9039e-02, 1.6466e-01, 1.6849e-01, 4.9420e-02, 1.9690e-01, 1.4891e-01,\n",
       "         8.9283e-02, 1.0402e-02, 9.2379e-02, 0.0000e+00, 4.6388e-02, 1.7635e-02,\n",
       "         2.2059e-01, 0.0000e+00, 1.1609e-02, 2.5094e-02, 6.9712e-02, 9.7280e-02,\n",
       "         8.1212e-02, 5.7332e-02, 9.3212e-03, 3.1089e-02, 1.9088e-02, 4.4550e-03,\n",
       "         2.1000e-02, 2.1543e-02, 1.0300e-01, 2.5453e-01, 5.2414e-02, 7.1322e-02,\n",
       "         1.9409e-02, 1.9705e-01, 6.0074e-02, 5.4063e-02, 5.6500e-02, 2.3870e-01,\n",
       "         7.4072e-03, 1.3736e-02, 3.1052e-02, 5.7699e-03, 1.3672e-01, 4.5651e-02,\n",
       "         2.0733e-02, 9.7924e-03, 3.1564e-02, 2.0148e-02, 5.7234e-02, 2.0617e-01,\n",
       "         6.8804e-02, 5.8820e-02, 2.4648e-02, 1.3774e-02, 1.2160e-01, 4.3906e-02,\n",
       "         1.9806e-01, 6.7243e-02, 4.1133e-02, 4.3489e-02, 6.3553e-02, 4.7229e-02,\n",
       "         1.3348e-02, 8.0872e-02, 3.0954e-01, 2.9502e-01, 8.3514e-02, 4.1568e-02,\n",
       "         1.9589e-01, 5.1518e-02, 5.5920e-02, 4.4402e-02, 3.0954e-03, 6.0470e-02,\n",
       "         0.0000e+00, 8.1327e-02, 0.0000e+00, 1.5507e-02, 2.6222e-02, 4.3384e-02,\n",
       "         4.1169e-02, 3.3884e-02, 7.4005e-03, 1.5733e-02, 7.0675e-02, 2.7104e-02,\n",
       "         1.9947e-02, 9.3799e-02, 9.1498e-02, 6.3277e-02],\n",
       "        [2.1637e-03, 5.6173e-02, 2.4168e-03, 8.9651e-02, 2.1298e-04, 2.8308e-02,\n",
       "         4.1119e-02, 1.1675e-01, 7.1863e-03, 4.0986e-03, 8.3381e-02, 5.1643e-04,\n",
       "         0.0000e+00, 1.6969e-01, 3.5873e-02, 1.7868e-01, 1.5187e-01, 5.1853e-02,\n",
       "         7.4071e-02, 8.9432e-02, 1.2409e-01, 1.7727e-02, 1.3002e-03, 7.8306e-02,\n",
       "         9.7993e-03, 5.5455e-02, 5.7457e-02, 1.5740e-01, 1.3635e-01, 8.6559e-02,\n",
       "         2.5341e-02, 2.2799e-02, 1.7250e-02, 9.3569e-02, 6.8358e-02, 1.2161e-01,\n",
       "         9.0261e-03, 2.1047e-01, 0.0000e+00, 2.4948e-01, 1.4032e-02, 9.1529e-02,\n",
       "         9.3299e-02, 6.2851e-02, 4.1823e-02, 5.7413e-02, 1.7075e-01, 4.8696e-02,\n",
       "         1.7457e-01, 1.8407e-02, 1.2855e-01, 2.6912e-01, 0.0000e+00, 4.6633e-02,\n",
       "         5.1966e-02, 1.0064e-01, 1.9858e-03, 1.0436e-01, 1.1219e-01, 3.8376e-02,\n",
       "         5.5009e-02, 8.3680e-02, 7.6586e-02, 8.5318e-02, 1.5709e-02, 2.0751e-02,\n",
       "         4.8510e-02, 1.2681e-03, 1.8371e-01, 6.1715e-02, 4.2976e-03, 3.9690e-02,\n",
       "         5.8579e-03, 5.2415e-02, 8.1786e-02, 0.0000e+00, 1.4742e-01, 7.1848e-02,\n",
       "         2.1053e-01, 2.8497e-02, 5.6941e-02, 1.4709e-02, 7.8328e-03, 3.9209e-02,\n",
       "         1.7898e-02, 1.3213e-01, 7.4558e-02, 3.0834e-02, 3.8713e-03, 4.8476e-03,\n",
       "         9.0005e-02, 1.3542e-01, 6.1881e-03, 6.8052e-03, 0.0000e+00, 1.7281e-01,\n",
       "         1.0028e-01, 2.4651e-02, 1.4756e-01, 1.5761e-02, 2.3784e-01, 9.9782e-03,\n",
       "         2.1728e-01, 3.4102e-02, 2.4198e-02, 0.0000e+00, 2.8123e-02, 5.2265e-02,\n",
       "         5.5225e-02, 6.2717e-03, 7.8267e-02, 2.1924e-03, 6.4880e-03, 6.0956e-02,\n",
       "         5.6360e-02, 7.5808e-02, 3.8555e-02, 6.8755e-02, 6.5663e-02, 5.5068e-02,\n",
       "         1.1576e-02, 4.7026e-02, 9.2093e-02, 6.0991e-02, 1.2335e-01, 1.8337e-01,\n",
       "         3.2180e-02, 1.7659e-02, 5.8950e-02, 2.1033e-02, 2.8476e-02, 0.0000e+00,\n",
       "         1.0319e-01, 1.9359e-02, 1.6268e-01, 2.5576e-02, 0.0000e+00, 1.4845e-01,\n",
       "         1.0388e-01, 9.8256e-02, 1.4285e-02, 9.5485e-02, 3.7005e-04, 5.4144e-02,\n",
       "         1.6888e-01, 2.4885e-02, 4.5857e-02, 1.4355e-02, 3.6270e-02, 1.3142e-02,\n",
       "         2.1285e-03, 1.6397e-01, 2.7198e-03, 1.5262e-02, 0.0000e+00, 0.0000e+00,\n",
       "         3.1353e-02, 8.0366e-02, 9.1514e-02, 1.5916e-01, 1.8205e-01, 1.6471e-01,\n",
       "         2.4982e-01, 2.2689e-02, 1.3521e-02, 4.4591e-02, 7.0491e-02, 1.7426e-01,\n",
       "         1.8168e-01, 0.0000e+00, 5.0444e-03, 1.2316e-01, 2.8928e-02, 4.2779e-02,\n",
       "         1.3247e-01, 4.5793e-02, 1.6471e-02, 6.8980e-02, 6.2197e-02, 8.4051e-03,\n",
       "         8.2532e-02, 5.4959e-02, 2.6864e-02, 6.0061e-02, 1.5850e-01, 5.2022e-02,\n",
       "         2.1732e-01, 2.1084e-01, 3.9348e-03, 2.2700e-02, 8.7317e-03, 1.0721e-01,\n",
       "         0.0000e+00, 2.8622e-02, 5.5242e-03, 3.9226e-03, 1.1912e-01, 3.7788e-02,\n",
       "         6.2248e-02, 2.1363e-01, 0.0000e+00, 6.8941e-03, 9.3194e-02, 7.1075e-02,\n",
       "         5.6970e-02, 8.3482e-02, 3.4231e-03, 3.1361e-01, 0.0000e+00, 6.6479e-02,\n",
       "         1.2324e-01, 0.0000e+00, 1.3991e-01, 1.1591e-01, 1.1108e-01, 4.3936e-02,\n",
       "         4.0200e-02, 8.9927e-02, 1.9995e-01, 4.6730e-02, 1.4247e-02, 4.9893e-02,\n",
       "         4.8978e-03, 3.0915e-01, 6.1490e-03, 7.7423e-02, 3.8848e-03, 2.4491e-03,\n",
       "         3.6361e-02, 1.9164e-01, 1.1381e-01, 1.5922e-01, 5.5226e-03, 6.1136e-03,\n",
       "         2.9944e-02, 2.9660e-02, 1.6062e-01, 3.9016e-02, 1.8625e-01, 6.1797e-03,\n",
       "         2.5738e-02, 8.6629e-02, 3.0534e-02, 7.2221e-02, 1.3071e-02, 9.8772e-02,\n",
       "         2.2174e-01, 6.0289e-02, 1.0370e-01, 4.6139e-02, 5.1458e-02, 7.5040e-02,\n",
       "         0.0000e+00, 2.9126e-02, 1.6810e-02, 3.6629e-02, 1.0015e-01, 5.6183e-02,\n",
       "         2.4805e-01, 1.6232e-01, 1.3595e-02, 1.2791e-01, 5.5562e-01, 7.6644e-02,\n",
       "         4.8958e-02, 6.9729e-02, 5.8108e-03, 1.0108e-01, 1.2480e-01, 6.6637e-02,\n",
       "         4.0057e-02, 9.8278e-02, 3.9396e-02, 6.5763e-02, 4.3088e-02, 6.3926e-02,\n",
       "         2.9134e-02, 1.0787e-01, 1.2058e-01, 1.9111e-02, 7.8014e-02, 4.9581e-02,\n",
       "         1.0995e-03, 2.5684e-01, 1.1104e-02, 1.7844e-01, 7.9069e-02, 8.6813e-03,\n",
       "         5.7375e-04, 6.6476e-02, 2.8556e-01, 1.4984e-01, 2.8854e-02, 6.3155e-02,\n",
       "         3.0883e-02, 2.5372e-02, 4.3657e-02, 3.4898e-02, 1.4087e-01, 9.8740e-02,\n",
       "         3.9035e-02, 5.8424e-02, 2.5227e-01, 6.1693e-03, 4.3617e-02, 1.0103e-01,\n",
       "         1.0318e-01, 5.9636e-02, 6.4697e-02, 2.6359e-01, 5.5078e-03, 3.3480e-03,\n",
       "         1.1683e-01, 1.4165e-01, 1.5595e-01, 4.4349e-02, 1.4824e-01, 1.8490e-01,\n",
       "         9.0703e-02, 9.9500e-03, 9.8867e-02, 3.2987e-03, 6.1656e-02, 1.4093e-02,\n",
       "         1.8671e-01, 0.0000e+00, 1.9576e-02, 1.7915e-02, 6.1280e-02, 1.3041e-01,\n",
       "         7.6207e-02, 4.3338e-02, 1.7021e-02, 2.1405e-02, 5.6147e-03, 4.4169e-03,\n",
       "         1.2551e-02, 6.3321e-02, 8.5113e-02, 1.9124e-01, 7.8993e-02, 5.9805e-02,\n",
       "         1.9689e-02, 2.0181e-01, 2.8871e-02, 3.3622e-02, 4.8265e-02, 2.6696e-01,\n",
       "         6.7967e-03, 1.1181e-02, 3.0573e-02, 1.2091e-02, 1.0141e-01, 2.9503e-02,\n",
       "         2.7381e-02, 1.9544e-02, 4.5756e-02, 2.2317e-02, 5.0768e-02, 2.0591e-01,\n",
       "         9.0829e-02, 8.4061e-02, 2.8163e-02, 2.6540e-02, 1.1936e-01, 2.8084e-02,\n",
       "         1.5027e-01, 8.0033e-02, 4.7567e-02, 2.3042e-02, 4.4255e-02, 3.7203e-04,\n",
       "         2.1020e-02, 5.2695e-02, 2.5707e-01, 1.9472e-01, 9.8306e-02, 8.5092e-02,\n",
       "         1.5178e-01, 4.1089e-02, 4.5957e-02, 5.3479e-02, 9.8697e-03, 3.4919e-02,\n",
       "         2.8767e-03, 1.1987e-01, 0.0000e+00, 2.8336e-02, 3.5090e-02, 6.5878e-02,\n",
       "         5.0678e-02, 3.1648e-02, 1.1166e-02, 4.4500e-03, 6.8720e-02, 1.9659e-02,\n",
       "         3.9841e-02, 5.6381e-02, 5.7391e-02, 8.4370e-02]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_pt = DocEncoder()\n",
    "dim1 = 30\n",
    "dim2 = 300\n",
    "x = torch.randn((2, dim1, dim2))\n",
    "\n",
    "d_pt.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_18 (None, 50, 400)\n",
      "attention_4 (None, 50, 400)\n",
      "lambda_3 (None, 20, 400)\n",
      "dropout_19 (None, 50, 400)\n",
      "gru_3 (None, 400)\n",
      "model_17 (None, 400)\n",
      "reshape_2 (None, 1, 400)\n",
      "reshape_1 (None, 1, 400)\n",
      "concatenate_1 (None, 2, 400)\n",
      "model_18 (None, 400)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_user_encoder():\n",
    "    news_vecs_input = Input(shape=(50,400), dtype='float32')\n",
    "    \n",
    "    news_vecs = Dropout(0.2)(news_vecs_input)\n",
    "    gru_input = keras.layers.Lambda(lambda x:x[:,-15:,:])(news_vecs)\n",
    "    vec1 = GRU(400)(gru_input)\n",
    "    vecs2 = Attention(20,20)([news_vecs]*3)\n",
    "    vec2 = tfAttentivePooling(50,400)(vecs2)\n",
    "\n",
    "    user_vecs2 = Attention(20,20)([news_vecs_input]*3)\n",
    "    user_vecs2 = Dropout(0.2)(user_vecs2)\n",
    "    user_vec2 = tfAttentivePooling(50,400)(user_vecs2)\n",
    "    user_vec2 = keras.layers.Reshape((1,400))(user_vec2)\n",
    "        \n",
    "    user_vecs1 = Lambda(lambda x:x[:,-20:,:])(news_vecs_input)\n",
    "    user_vec1 = GRU(400)(user_vecs1)\n",
    "    user_vec1 = keras.layers.Reshape((1,400))(user_vec1)\n",
    "\n",
    "    user_vecs = keras.layers.Concatenate(axis=-2)([user_vec1,user_vec2])\n",
    "    vec = tfAttentivePooling(2,400)(user_vecs)\n",
    "        \n",
    "    sentEncodert = Model(news_vecs_input, vec)\n",
    "    return sentEncodert\n",
    "\n",
    "u = get_user_encoder()\n",
    "print_model(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VecTail(nn.Module):\n",
    "    def __init__(self, n):\n",
    "        super(VecTail, self).__init__()\n",
    "        self.n = n\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:,-self.n,:]\n",
    "\n",
    "class UserEncoder(nn.Module):\n",
    "    def __init__(self):        \n",
    "        super(UserEncoder,self).__init__()\n",
    "        # news_vecs_input = Input(shape=(50,400), dtype='float32')\n",
    "        #self.dropout1 = nn.Dropout(0.2)\n",
    "        #self.tail = VecTail(15)\n",
    "        #self.gru = nn.GRU(400, 400)\n",
    "        #self.attention = nn.MultiheadAttention(400, 20)\n",
    "        #self.pool = AttentivePooling(50, 400)\n",
    "        self.attention2 = nn.MultiheadAttention(400, 20)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.pool2 = AttentivePooling(50, 400)\n",
    "        self.tail2 = VecTail(20)\n",
    "        self.gru2 = nn.GRU(400,400)\n",
    "        self.pool3 = AttentivePooling(2, 400)\n",
    "\n",
    "    def forward(self, news_vecs_input):    \n",
    "        #news_vecs =self.dropout1(news_vecs_input)\n",
    "        #gru_input = self.tail(news_vecs)\n",
    "        #vec1 = self.gru(gru_input)\n",
    "        #vecs2 = self.attention(*[news_vecs]*3)\n",
    "        #vec2 = self.pool(vecs2)\n",
    "    \n",
    "        user_vecs2, _u_weights = self.attention2(*[news_vecs_input]*3)\n",
    "        user_vecs2 = self.dropout2(user_vecs2)\n",
    "        user_vec2 = self.pool2(user_vecs2)\n",
    "        print(user_vec2.shape)\n",
    "        #user_vec2 = keras.layers.Reshape((1,400))(user_vec2)\n",
    "        #user_vec2 = user_vec2.unsqueeze(1)\n",
    "\n",
    "        user_vecs1 = self.tail2(news_vecs_input)\n",
    "        user_vec1, _u_hidden = self.gru2(user_vecs1)\n",
    "        #user_vec1 = keras.layers.Reshape((1,400))(user_vec1)\n",
    "        #user_vec1 = user_vec1.unsqueeze(1)\n",
    "        \n",
    "        user_vecs = torch.stack([user_vec1, user_vec2], dim=1) #keras.layers.Concatenate(axis=-2)([user_vec1,user_vec2])\n",
    "        print(user_vecs.shape)\n",
    "        vec = self.pool3(user_vecs)\n",
    "        print(vec.shape)\n",
    "        return vec\n",
    "        \n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 400])\n",
      "torch.Size([2, 2, 400])\n",
      "torch.Size([2, 400])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.4103e-01, -2.2155e-01,  5.5414e-02, -1.0528e-01, -2.6470e-02,\n",
       "          5.6696e-02,  1.4110e-01, -6.1008e-02,  4.1602e-02, -1.0160e-01,\n",
       "         -2.2682e-02,  2.5239e-01,  2.3476e-02,  4.7963e-02, -5.2091e-02,\n",
       "          1.6705e-01,  9.3662e-02, -2.2356e-01,  6.5630e-02, -9.0097e-02,\n",
       "          1.7363e-01,  1.4190e-01,  1.9042e-01,  2.0884e-01,  1.0162e-03,\n",
       "          1.2115e-01, -2.8425e-01, -1.1501e-01, -7.7650e-02,  4.8579e-02,\n",
       "          3.3464e-02,  1.2099e-02,  1.1184e-01, -2.0019e-01, -1.2009e-01,\n",
       "         -6.9716e-02,  2.5229e-01, -2.3580e-02, -1.8622e-01,  6.8790e-02,\n",
       "         -1.5805e-02, -8.6447e-02, -2.0113e-01,  6.0035e-02, -4.2379e-03,\n",
       "         -1.2508e-01,  9.4129e-02,  1.6371e-01,  4.1479e-02,  1.8373e-01,\n",
       "         -2.6810e-01, -2.2285e-01, -4.2221e-03,  3.3998e-01, -2.5080e-01,\n",
       "          2.3551e-03,  2.1028e-01, -2.1904e-01,  3.0840e-01, -2.1326e-01,\n",
       "          2.3559e-02,  5.4087e-02, -3.3355e-02, -9.3945e-03,  4.3261e-02,\n",
       "          1.8866e-01, -7.0549e-02, -3.6096e-02, -1.7904e-01, -1.2825e-01,\n",
       "         -1.2870e-01,  1.2201e-01,  2.9698e-02,  2.0166e-01,  2.2713e-01,\n",
       "         -1.2087e-01,  4.8706e-02,  7.0966e-03,  2.9630e-02, -1.9588e-01,\n",
       "         -1.9810e-01, -6.9874e-03, -7.8450e-02, -4.6299e-02, -4.1247e-02,\n",
       "         -1.1202e-01, -3.3909e-02, -7.7828e-02, -4.8182e-02, -1.1457e-01,\n",
       "          4.9834e-02,  7.3368e-03,  1.1365e-02, -5.9326e-02,  0.0000e+00,\n",
       "          1.3917e-01,  6.1241e-02, -4.6558e-02,  1.2597e-01, -8.2075e-02,\n",
       "          2.8428e-02, -9.1655e-02, -2.4140e-02,  4.8942e-02,  0.0000e+00,\n",
       "          1.2600e-01,  2.4062e-02,  1.3351e-01,  1.3665e-01, -3.1219e-01,\n",
       "          1.7625e-01,  3.8482e-03, -8.1308e-02, -8.7785e-02, -5.7163e-02,\n",
       "          4.2079e-02,  8.0967e-02,  1.9497e-02, -1.7064e-01, -2.5494e-01,\n",
       "         -5.5810e-03,  4.6897e-02,  2.0411e-02, -4.4244e-02, -1.8779e-01,\n",
       "         -1.5476e-01, -2.6485e-01, -8.3173e-03, -1.3020e-01, -4.1881e-02,\n",
       "          3.3821e-02, -2.9784e-02, -6.0834e-02,  1.2163e-01, -1.2629e-01,\n",
       "          2.6075e-01, -1.1029e-02, -1.1854e-01, -1.0621e-01,  8.1975e-02,\n",
       "          8.7602e-02,  1.5154e-02,  2.4274e-01, -8.9667e-03, -2.0549e-02,\n",
       "          1.0971e-01,  1.6542e-01,  5.7329e-02, -3.9678e-02,  1.4805e-01,\n",
       "         -1.9294e-01,  0.0000e+00,  6.9859e-02,  0.0000e+00, -2.0647e-01,\n",
       "          3.9367e-02,  3.9337e-02, -1.1352e-01, -2.1522e-01, -5.2652e-02,\n",
       "          4.6985e-02,  1.9314e-01,  1.1020e-01,  4.8141e-02, -4.5306e-03,\n",
       "          1.4766e-01,  4.6665e-02, -1.1283e-01, -9.8110e-02,  2.5800e-02,\n",
       "          2.1799e-01, -1.3993e-02,  1.4652e-01,  2.3235e-02, -2.4399e-03,\n",
       "          3.7993e-01, -1.8789e-01,  8.5496e-02, -1.3005e-01, -5.7477e-02,\n",
       "          4.6486e-03, -1.7911e-01, -2.4201e-02, -5.7174e-02,  2.0473e-01,\n",
       "         -1.8774e-01,  2.9042e-01, -2.4103e-01, -6.7834e-02, -2.3344e-01,\n",
       "          1.6862e-02, -2.6764e-02,  1.2513e-02, -1.9299e-01,  2.4493e-02,\n",
       "         -1.1563e-01,  3.8568e-02,  0.0000e+00, -1.0881e-01,  3.0199e-02,\n",
       "          2.3874e-01, -1.0173e-01, -2.5434e-02,  1.9290e-01,  0.0000e+00,\n",
       "         -1.1898e-01,  1.6763e-01, -3.1438e-01, -6.2120e-02,  1.0322e-01,\n",
       "          2.9803e-02,  3.8481e-01,  3.0468e-01,  4.1391e-02, -1.7070e-01,\n",
       "          3.2700e-02, -3.5447e-02,  2.3525e-01,  1.3457e-01,  9.7273e-02,\n",
       "          1.8123e-01,  6.2549e-02, -1.0910e-02, -2.1563e-01,  1.1572e-03,\n",
       "         -3.6304e-02, -5.7359e-02,  2.8089e-02,  0.0000e+00, -6.9529e-02,\n",
       "          1.4421e-01,  1.5984e-01, -1.3648e-01,  2.0734e-01,  5.8375e-02,\n",
       "          2.4852e-02,  5.6532e-02,  5.4312e-02,  1.4958e-01, -4.8159e-02,\n",
       "          8.6368e-02, -6.2958e-02,  1.1271e-03,  1.6991e-01, -6.6728e-02,\n",
       "         -2.7979e-03, -4.2589e-02,  1.4760e-01,  1.3044e-01, -1.6674e-01,\n",
       "          2.4274e-01,  1.8927e-02,  3.0583e-02,  1.5025e-01,  2.3618e-01,\n",
       "         -1.7271e-01, -5.9944e-02,  1.4613e-01, -1.4349e-01,  8.7436e-02,\n",
       "         -2.7261e-01,  0.0000e+00, -2.3842e-02, -2.0491e-01, -2.3772e-01,\n",
       "         -6.4814e-02, -1.6960e-01, -1.2824e-01,  4.3827e-02, -6.9515e-03,\n",
       "          2.5512e-01, -2.9100e-01, -7.8432e-03, -1.9756e-01, -1.9543e-01,\n",
       "          5.4881e-02, -1.3922e-01,  1.1529e-01,  1.4964e-02,  0.0000e+00,\n",
       "          4.9291e-02, -1.3538e-01,  3.3959e-02,  3.1576e-02,  6.3268e-03,\n",
       "          4.8946e-02, -7.0975e-03, -5.0171e-02, -2.7231e-01, -1.1933e-02,\n",
       "          3.2053e-02, -1.9092e-01,  3.5113e-03, -3.0511e-02,  8.4789e-02,\n",
       "         -1.9591e-01,  7.2998e-02,  2.3639e-02, -3.3461e-02, -1.1589e-01,\n",
       "          3.1988e-02, -2.0014e-02,  0.0000e+00, -1.1657e-04,  3.8221e-02,\n",
       "         -1.8012e-01, -1.4616e-01,  1.0624e-01, -1.4002e-01, -5.0237e-02,\n",
       "         -1.8225e-01, -7.9270e-02,  1.2592e-01,  3.6879e-02,  6.9573e-02,\n",
       "          3.6324e-01,  3.3272e-01,  5.5427e-02,  4.6544e-02,  8.8497e-02,\n",
       "         -1.4567e-01, -6.0411e-02, -8.0994e-02, -9.7017e-02, -1.0540e-01,\n",
       "          3.9887e-02,  6.0869e-02,  2.1179e-01,  1.8071e-01, -7.5909e-02,\n",
       "          7.4320e-02,  1.2647e-01,  1.5020e-02,  0.0000e+00, -8.3536e-02,\n",
       "          1.4146e-01, -8.0473e-03,  1.0084e-03,  1.4831e-01,  6.3720e-02,\n",
       "          2.0957e-01,  1.5733e-01, -7.2125e-02,  2.9758e-02, -1.3182e-01,\n",
       "          1.4958e-02,  1.0771e-01, -1.2962e-03, -6.0939e-02, -2.0338e-01,\n",
       "         -2.2692e-02,  3.5431e-02,  2.9584e-01, -2.4882e-01,  4.8964e-02,\n",
       "          1.6058e-01,  6.8126e-03, -9.4198e-02,  7.6755e-02, -2.5030e-02,\n",
       "          5.0980e-02, -1.2915e-02,  0.0000e+00, -1.5333e-01,  8.6005e-02,\n",
       "          1.2817e-01, -2.5200e-02, -9.7102e-05,  2.0436e-02,  2.5384e-02,\n",
       "          3.4857e-02,  3.9730e-02,  2.0134e-01,  4.8597e-02, -8.9430e-02,\n",
       "          1.4184e-01, -2.8819e-01,  8.4186e-04, -2.2180e-01, -1.4267e-01,\n",
       "          5.8067e-02,  1.8735e-01, -6.9208e-02, -2.9167e-02, -1.0245e-01,\n",
       "          0.0000e+00,  2.6738e-02, -1.0690e-01,  1.0320e-01,  0.0000e+00,\n",
       "          3.0957e-02,  1.6124e-01,  2.1860e-02, -1.2551e-01,  2.0308e-01,\n",
       "          2.3077e-01,  0.0000e+00, -1.1869e-01,  1.5288e-01, -2.0409e-01],\n",
       "        [ 0.0000e+00, -2.1849e-01,  6.7631e-02, -1.4858e-01, -1.3075e-02,\n",
       "          6.1895e-02,  5.9149e-02, -1.0384e-01,  4.3498e-02,  3.5307e-02,\n",
       "         -8.5497e-02,  6.4409e-02, -1.0927e-01, -5.3516e-03, -1.2032e-01,\n",
       "          1.3380e-01,  8.6383e-03, -3.2766e-01, -7.4364e-02, -4.6580e-02,\n",
       "          3.1217e-01, -2.6591e-02,  9.5792e-02, -9.7522e-02,  2.1258e-02,\n",
       "         -2.1776e-01, -7.9266e-02,  2.1759e-01, -1.3323e-01,  1.4185e-02,\n",
       "         -1.8509e-02, -1.0298e-01,  0.0000e+00, -6.6684e-02, -2.2174e-02,\n",
       "         -1.5065e-01,  3.4169e-02, -1.9844e-01, -2.2499e-01,  6.6853e-02,\n",
       "          2.7388e-02,  1.3126e-01, -3.7400e-01,  1.1307e-02, -1.0183e-01,\n",
       "         -2.7230e-01,  1.0483e-01,  8.8759e-02, -9.3701e-02,  2.4643e-01,\n",
       "          1.2355e-01,  7.6698e-03, -3.2705e-02,  3.8924e-01, -2.6275e-01,\n",
       "         -1.6703e-01, -2.1045e-01, -7.4057e-02,  6.9169e-02, -8.7644e-02,\n",
       "         -2.1374e-02,  0.0000e+00, -3.4336e-03,  0.0000e+00, -2.8254e-01,\n",
       "         -2.2248e-02, -6.6772e-02, -8.7895e-03, -8.5213e-02,  1.4967e-01,\n",
       "          3.4952e-02,  8.3736e-02, -2.2869e-01, -3.8840e-02,  1.0909e-01,\n",
       "          2.0144e-02, -3.8290e-02,  2.5844e-01, -1.7821e-01, -1.5752e-01,\n",
       "          6.3551e-02,  3.8261e-02, -2.1611e-02,  3.8356e-03,  0.0000e+00,\n",
       "         -1.9044e-01, -1.9560e-01,  5.5464e-02, -3.4645e-01, -2.3882e-01,\n",
       "          2.7941e-01,  2.8463e-01,  7.7657e-02,  3.1285e-02, -2.8210e-01,\n",
       "          6.5994e-02,  5.3704e-02,  4.0904e-02, -2.8548e-01,  1.2361e-01,\n",
       "          6.6839e-02, -3.2138e-02, -1.4062e-01, -1.1267e-01, -1.0861e-01,\n",
       "          2.1737e-02,  6.2914e-02,  8.8019e-02,  5.3339e-02,  1.2031e-01,\n",
       "          2.7921e-01,  4.3989e-02, -9.5797e-02,  5.2951e-02,  4.4414e-02,\n",
       "          2.6011e-03,  6.8184e-03,  1.0741e-01,  6.6646e-02, -2.5929e-01,\n",
       "          1.3124e-01,  2.6022e-01, -9.3734e-02, -2.2296e-03, -7.3732e-02,\n",
       "          1.7241e-02, -5.6040e-02,  8.7545e-02, -9.2135e-02,  1.6608e-02,\n",
       "          9.6243e-02, -1.4479e-01,  2.0421e-01,  3.3809e-02,  2.6213e-03,\n",
       "          2.8118e-01, -1.6900e-01,  1.3478e-01, -1.0793e-01,  1.0223e-01,\n",
       "         -6.4879e-02, -4.1551e-02, -5.2215e-03,  9.3696e-02,  1.5760e-01,\n",
       "         -2.1986e-01,  7.2364e-03, -1.1660e-01,  2.1809e-01,  1.0695e-01,\n",
       "         -2.8987e-01, -1.8515e-01,  2.5233e-02, -1.9895e-02,  5.7585e-02,\n",
       "          8.0498e-02,  2.0688e-01, -2.1243e-01, -1.7765e-02, -1.6813e-01,\n",
       "          1.6748e-01, -4.8016e-02,  3.5463e-01, -7.5273e-02, -4.3044e-02,\n",
       "          9.4991e-02,  2.4390e-01, -2.7929e-02, -2.0548e-01,  1.3778e-01,\n",
       "          9.9247e-02, -2.5491e-02, -1.8185e-01,  1.9445e-01, -1.4778e-02,\n",
       "         -7.7861e-03, -3.2294e-01, -7.0095e-02,  1.1736e-01, -7.2800e-02,\n",
       "         -1.4940e-01,  1.4106e-01, -2.0570e-01,  5.1666e-02,  1.0346e-01,\n",
       "         -1.6435e-01,  3.6416e-02, -1.6679e-01, -3.4507e-01, -1.2981e-01,\n",
       "         -5.0735e-02, -2.3608e-01,  1.8345e-02, -4.6861e-02,  1.1625e-02,\n",
       "          2.1193e-02, -1.7830e-01,  9.4633e-02,  3.4238e-02, -3.1740e-01,\n",
       "         -5.3639e-02, -1.4479e-01,  1.7839e-01,  1.6559e-02,  3.2622e-01,\n",
       "         -4.8390e-03,  1.5582e-02, -8.6949e-02,  4.0813e-02, -1.0152e-01,\n",
       "          5.5007e-02,  5.8557e-02,  3.3884e-01,  2.9698e-02, -5.8562e-02,\n",
       "         -4.5326e-02, -1.5794e-01,  1.7267e-01, -5.2293e-03, -1.8908e-01,\n",
       "          2.6909e-01,  2.7238e-02,  8.7314e-02,  1.2453e-02,  8.2332e-02,\n",
       "          3.8441e-02,  0.0000e+00,  2.7437e-02, -1.0781e-01,  2.9191e-02,\n",
       "         -2.2957e-01,  1.8218e-01, -8.3335e-02,  2.7817e-01,  1.1081e-01,\n",
       "         -9.8439e-02,  1.0904e-01,  6.7961e-03, -2.0221e-02,  0.0000e+00,\n",
       "          7.5104e-02,  5.0781e-02, -2.0350e-02, -1.5018e-02, -1.0307e-01,\n",
       "          2.2727e-01,  1.3115e-01,  0.0000e+00,  1.7393e-01, -9.4046e-02,\n",
       "          6.4337e-02,  1.7086e-02,  3.6899e-01, -2.7603e-02,  7.3168e-02,\n",
       "         -2.4470e-01, -2.3281e-01,  6.3238e-02, -2.2654e-01,  2.3728e-01,\n",
       "          0.0000e+00,  1.5553e-01,  0.0000e+00, -2.1412e-01, -4.7805e-01,\n",
       "          1.2055e-01, -3.5589e-01,  2.3581e-01,  2.9076e-01,  1.1370e-01,\n",
       "         -8.4602e-02, -3.1412e-01,  3.4868e-01,  0.0000e+00, -2.4509e-01,\n",
       "          2.3905e-02,  2.5599e-02, -9.4217e-02,  2.4095e-02, -6.4080e-02,\n",
       "         -1.8213e-01,  7.2809e-02,  1.2601e-02, -2.4944e-01,  8.3196e-03,\n",
       "         -1.6197e-01,  1.1950e-01, -2.4701e-02, -1.0240e-01, -1.7070e-01,\n",
       "         -3.9482e-03, -3.1217e-01,  1.3158e-02, -2.5623e-02, -1.5455e-02,\n",
       "          1.6678e-01, -1.4588e-01,  1.4670e-01, -1.6606e-01, -4.2974e-02,\n",
       "          3.0990e-01,  1.2845e-02,  1.5718e-01,  2.6319e-01,  1.4422e-01,\n",
       "         -1.2364e-01, -1.2655e-01,  2.5295e-01, -1.2133e-02, -1.6957e-01,\n",
       "         -1.3596e-01, -7.5435e-02,  9.7708e-02,  2.5415e-02, -5.0066e-02,\n",
       "          3.0510e-01,  2.4001e-02, -1.2157e-01,  4.1996e-02,  2.1881e-01,\n",
       "         -1.7258e-02, -7.8131e-02, -3.0166e-01,  1.0328e-01, -1.6988e-01,\n",
       "          2.2574e-01, -5.1151e-02,  1.4882e-01,  1.1286e-01,  1.5026e-01,\n",
       "          2.2004e-01,  2.1718e-01,  6.9251e-03, -2.1791e-01,  1.0098e-01,\n",
       "         -4.9446e-02,  3.0232e-01,  4.4598e-02, -1.6254e-01, -1.1944e-01,\n",
       "          1.3180e-02,  9.1797e-02,  7.3293e-02, -2.5271e-01,  2.8639e-02,\n",
       "          2.3110e-02,  4.7534e-02, -1.3122e-01,  2.8290e-02,  1.0087e-02,\n",
       "          5.8855e-02, -3.4297e-02,  7.2156e-03, -2.2105e-01,  3.2689e-01,\n",
       "          2.2611e-01, -4.9600e-02, -3.1435e-03, -5.8426e-03,  3.7821e-01,\n",
       "          5.3201e-03,  1.1474e-02, -3.7040e-02, -1.1191e-01, -1.6135e-01,\n",
       "          1.6417e-01,  4.9375e-02,  1.1825e-01, -1.6527e-01,  7.0340e-02,\n",
       "         -1.4390e-01,  9.7731e-03,  6.4699e-02,  7.8973e-02,  2.3243e-02,\n",
       "         -5.3942e-02,  5.9970e-03,  7.9238e-02, -2.9678e-01,  4.6325e-02,\n",
       "          2.2517e-01,  1.3791e-01,  2.2716e-02,  0.0000e+00, -3.6769e-02,\n",
       "          2.3395e-01, -1.9008e-02,  1.4105e-01, -2.9954e-02, -1.7237e-01,\n",
       "          1.4192e-04, -7.0107e-02,  0.0000e+00,  2.2148e-01, -4.3610e-02,\n",
       "         -1.2046e-01, -1.6433e-01,  1.2217e-01,  1.1147e-02, -3.2468e-01]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_pt = UserEncoder()\n",
    "dim1 = 50\n",
    "dim2 = 400\n",
    "x = torch.randn((2, dim1, dim2))\n",
    "\n",
    "u_pt.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "542f8443ae01e5e89658f1cc6305b82f52ba1db662565f568700972549f4b0db"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('fedrec')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
