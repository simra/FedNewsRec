{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rsim/anaconda3/envs/fedrec/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'model_pt' from '/home/rsim/FedNewsRec/code/model_pt.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import model_pt\n",
    "import importlib\n",
    "importlib.reload(model_pt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rsim/anaconda3/envs/fedrec/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/rsim/anaconda3/envs/fedrec/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/rsim/anaconda3/envs/fedrec/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/rsim/anaconda3/envs/fedrec/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/rsim/anaconda3/envs/fedrec/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/rsim/anaconda3/envs/fedrec/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "\n",
    "def tfDenseTest(dim1,dim2):\n",
    "    vecs_input = Input(shape=(dim1,dim2),dtype='float32')\n",
    "    user_att = Dense(100,activation='tanh', kernel_initializer= 'random_uniform', bias_initializer= 'random_uniform')(vecs_input)\n",
    "    model = Model(vecs_input,user_att)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_vecs torch.Size([2, 5, 10])\n",
      "dense1 torch.Size([2, 5, 200])\n",
      "flatten torch.Size([2, 5])\n",
      "softmax torch.Size([2, 5])\n",
      "dot torch.Size([2, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1211, -0.4455, -1.1638,  0.5727, -0.9729,  0.5442,  0.4008,  0.3014,\n",
       "          0.2522, -0.6350],\n",
       "        [-1.5661,  0.8745,  0.7691, -0.6407, -0.2056, -0.4551,  0.1764,  0.3775,\n",
       "          1.0522, -0.2276]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim1 = 5\n",
    "dim2 = 10\n",
    "x = torch.randn((2, dim1, dim2))\n",
    "\n",
    "pool = model_pt.AttentivePooling(dim1, dim2)\n",
    "\n",
    "#pool(x).shape\n",
    "\n",
    "\n",
    "pool(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "def tfAttentivePooling(dim1,dim2, name=None):\n",
    "    vecs_input = Input(shape=(dim1,dim2),dtype='float32')\n",
    "    user_vecs =Dropout(0.2)(vecs_input)\n",
    "    user_att = Dense(200,activation='tanh', kernel_initializer= 'random_uniform', bias_initializer= 'random_uniform')(user_vecs)\n",
    "    user_att = keras.layers.Flatten()(Dense(1, kernel_initializer= 'random_uniform', bias_initializer= 'random_uniform')(user_att))\n",
    "    user_att = Activation('softmax')(user_att)    \n",
    "    user_vec = keras.layers.Dot((1,1))([user_vecs,user_att])\n",
    "    model = Model(vecs_input,user_vec, name=name)\n",
    "    return model\n",
    "\n",
    "def testDot(dim1, dim2):\n",
    "    input = Input(shape=(dim1,dim2), dtype='float32')\n",
    "    output = keras.layers.Dot((2,2))([input, input])\n",
    "    return Model(input, output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-04 17:43:42.619010: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2022-06-04 17:43:42.641240: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2445435000 Hz\n",
      "2022-06-04 17:43:42.642282: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5600c3ac0880 executing computations on platform Host. Devices:\n",
      "2022-06-04 17:43:42.642314: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2022-06-04 17:43:42.991708: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \n",
      "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
      "pciBusID: 0001:00:00.0\n",
      "totalMemory: 14.56GiB freeMemory: 14.37GiB\n",
      "2022-06-04 17:43:42.991742: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
      "2022-06-04 17:43:42.993132: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-06-04 17:43:42.993146: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
      "2022-06-04 17:43:42.993162: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
      "2022-06-04 17:43:42.993218: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13983 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0001:00:00.0, compute capability: 7.5)\n",
      "2022-06-04 17:43:42.994864: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5600c3a98da0 executing computations on platform CUDA. Devices:\n",
      "2022-06-04 17:43:42.994881: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n"
     ]
    }
   ],
   "source": [
    "t = testDot(2,3)\n",
    "x_raw = [[[1,2,3],[4,5,6]]]\n",
    "x_tf = tf.convert_to_tensor(x_raw)\n",
    "t(x_tf).eval(session=tf.compat.v1.Session())\n",
    "x_pt = torch.tensor(x_raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_3 (None, 5, 10) []\n",
      "dropout_2 (None, 5, 10) []\n",
      "dense_3 (None, 5, 200) [(10, 200), (200,)]\n",
      "dense_4 (None, 5, 1) [(200, 1), (1,)]\n",
      "flatten_2 (None, 5) []\n",
      "activation_2 (None, 5) []\n",
      "dot_3 (None, 10) []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-04 17:44:41.944008: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
      "2022-06-04 17:44:41.944058: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-06-04 17:44:41.944063: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
      "2022-06-04 17:44:41.944066: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
      "2022-06-04 17:44:41.944115: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13983 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0001:00:00.0, compute capability: 7.5)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "import torch\n",
    "\n",
    "dim1 = 5\n",
    "dim2 = 10\n",
    "x = torch.randn((2, dim1, dim2))\n",
    "\n",
    "\n",
    "def tfAttentivePooling(dim1,dim2, name=None):\n",
    "    vecs_input = Input(shape=(dim1,dim2),dtype='float32')\n",
    "    user_vecs =Dropout(0.2)(vecs_input)\n",
    "    user_att = Dense(200,activation='tanh', kernel_initializer= 'random_uniform', bias_initializer= 'random_uniform')(user_vecs)\n",
    "    user_att = keras.layers.Flatten()(Dense(1, kernel_initializer= 'random_uniform', bias_initializer= 'random_uniform')(user_att))\n",
    "    user_att = Activation('softmax')(user_att)    \n",
    "    user_vec = keras.layers.Dot((1,1))([user_vecs,user_att])\n",
    "    model = Model(vecs_input,user_vec, name=name)\n",
    "    return model\n",
    "\n",
    "pool = tfAttentivePooling(dim1, dim2)\n",
    "\n",
    "def print_model(m):\n",
    "    for layer in m.layers:\n",
    "        try:\n",
    "            print(layer.name, layer.output_shape, [w.shape for w in layer.get_weights()]) \n",
    "        except:\n",
    "            print(layer.name, 'output0', layer.get_output_shape_at(0))\n",
    "\n",
    "print_model(pool)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    x_tf = tf.convert_to_tensor(x.numpy())\n",
    "    pool(x_tf).eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_3\n",
      "dropout_2\n",
      "dense_3\n",
      "dense_4\n",
      "flatten_2\n",
      "activation_2\n",
      "dot_3\n",
      "(2, 10)\n",
      "user_vecs torch.Size([2, 5, 10])\n",
      "dense1 torch.Size([2, 5, 200])\n",
      "flatten torch.Size([2, 5])\n",
      "softmax torch.Size([2, 5])\n",
      "dot torch.Size([2, 10])\n",
      "[[ 0.07939512  0.00312686  0.00488412  0.09010908  0.44272405  0.07363436\n",
      "  -0.1563558  -0.05927446  0.05221535  0.0496828 ]\n",
      " [ 0.26080972  0.12523896  0.07913249  0.07507315  0.03815147  0.13829823\n",
      "   0.0458912   0.30603275 -0.07538399 -0.12466186]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-04 17:50:03.660200: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
      "2022-06-04 17:50:03.660245: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-06-04 17:50:03.660250: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
      "2022-06-04 17:50:03.660254: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
      "2022-06-04 17:50:03.660330: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13983 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0001:00:00.0, compute capability: 7.5)\n"
     ]
    }
   ],
   "source": [
    "pool_pt = model_pt.AttentivePooling(dim1, dim2)\n",
    "\n",
    "for layer in pool.layers:\n",
    "    print(layer.name)\n",
    "    if layer.name=='dense_3':\n",
    "        weights = layer.get_weights()\n",
    "        pool_pt.dense.weight.data = torch.tensor(weights[0]).transpose(0,1)\n",
    "        pool_pt.dense.bias.data = torch.tensor(weights[1])\n",
    "    elif layer.name == 'dense_4':\n",
    "        weights = layer.get_weights()\n",
    "        pool_pt.dense2.weight.data = torch.tensor(weights[0]).transpose(0,1)\n",
    "        pool_pt.dense2.bias.data = torch.tensor(weights[1])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    x_tf = tf.convert_to_tensor(x.numpy())\n",
    "    y_tf = pool(x_tf).eval()\n",
    "    print(y_tf.shape)\n",
    "    with torch.no_grad():\n",
    "        pool_pt.eval()\n",
    "        print(y_tf-pool_pt(x).detach().numpy())\n",
    "    #print(pool_pt(x).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_3 (None, 30, 300)\n",
      "dropout_2 (None, 30, 300)\n",
      "conv1d_1 (None, 28, 400)\n",
      "dropout_3 (None, 28, 400)\n",
      "attention_1 (None, 28, 400)\n",
      "activation_2 (None, 28, 400)\n",
      "dropout_4 (None, 28, 400)\n",
      "model_3 (None, 400)\n"
     ]
    }
   ],
   "source": [
    "from models import Attention\n",
    "\n",
    "def get_doc_encoder():\n",
    "    sentence_input = Input(shape=(30,300), dtype='float32')\n",
    "    droped_vecs = Dropout(0.2)(sentence_input)\n",
    "\n",
    "    l_cnnt = Conv1D(400,3,activation='relu')(droped_vecs)\n",
    "    l_cnnt = Dropout(0.2)(l_cnnt)\n",
    "    l_cnnt = Attention(20,20)([l_cnnt,l_cnnt,l_cnnt])\n",
    "    l_cnnt = keras.layers.Activation('relu')(l_cnnt)\n",
    "    \n",
    "    droped_rep = Dropout(0.2)(l_cnnt)\n",
    "    title_vec = tfAttentivePooling(30,400)(droped_rep)\n",
    "    sentEncodert = Model(sentence_input, title_vec)\n",
    "    return sentEncodert\n",
    "\n",
    "d = get_doc_encoder()\n",
    "print_model(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Permute(nn.Module):\n",
    "    def __init__(self, *dims):\n",
    "        super(Permute, self).__init__()\n",
    "        self.dims = dims\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x.permute(*self.dims)\n",
    "\n",
    "class SwapTrailingAxes(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SwapTrailingAxes, self).__init__()\n",
    "        \n",
    "    def forward(self, x):        \n",
    "        return x.mT\n",
    "\n",
    "class DocEncoder(nn.Module):\n",
    "    def __init__(self):        \n",
    "        super(DocEncoder,self).__init__()\n",
    "        self.phase1 = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            SwapTrailingAxes(),            \n",
    "            nn.Conv1d(300,400,3),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            SwapTrailingAxes()\n",
    "        )\n",
    "        self.attention = nn.MultiheadAttention(400,20)\n",
    "        self.phase2 = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            AttentivePooling(30,400)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        l_cnnt = self.phase1(x)\n",
    "        print('doc_encoder:phase1',l_cnnt.shape)\n",
    "        l_cnnt, attention_weights = self.attention(l_cnnt, l_cnnt, l_cnnt)\n",
    "        print('doc_encoder:attention', l_cnnt.shape)\n",
    "        result = self.phase2(l_cnnt)\n",
    "        print('doc_encoder:phase2', result.shape)\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_encoder:phase1 torch.Size([2, 28, 400])\n",
      "doc_encoder:attention torch.Size([2, 28, 400])\n",
      "doc_encoder:phase2 torch.Size([2, 400])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[2.5350e-02, 6.0881e-02, 1.0347e-02, 1.4556e-02, 1.1262e-01, 1.6728e-02,\n",
       "         5.2783e-02, 9.6941e-03, 7.8505e-02, 4.0878e-02, 3.2404e-02, 9.0031e-02,\n",
       "         1.0173e-01, 9.2766e-02, 1.6397e-01, 7.4023e-02, 5.1577e-02, 5.8122e-02,\n",
       "         4.1079e-02, 1.9458e-02, 3.9305e-02, 6.1552e-02, 1.8461e-02, 4.3711e-02,\n",
       "         5.3473e-02, 2.3207e-02, 0.0000e+00, 1.2538e-01, 2.2131e-02, 8.8368e-02,\n",
       "         2.5095e-02, 1.6292e-01, 1.6799e-01, 6.5956e-03, 0.0000e+00, 1.2744e-01,\n",
       "         3.7824e-02, 2.8322e-01, 0.0000e+00, 7.3282e-02, 5.2872e-02, 7.2333e-04,\n",
       "         7.5436e-02, 1.6102e-01, 9.2541e-02, 1.9440e-03, 7.9293e-02, 3.1075e-02,\n",
       "         9.0353e-03, 3.7613e-02, 1.7275e-02, 7.7970e-02, 3.8281e-02, 1.6803e-02,\n",
       "         5.2499e-02, 6.6982e-02, 1.8000e-02, 1.1917e-01, 1.8935e-01, 3.4093e-02,\n",
       "         9.9588e-02, 2.3916e-02, 5.6093e-03, 6.4216e-02, 1.0940e-02, 1.2334e-01,\n",
       "         8.2217e-02, 1.4190e-01, 8.0195e-02, 3.8005e-02, 7.1861e-02, 4.2199e-02,\n",
       "         3.4656e-02, 6.0225e-03, 1.0285e-01, 7.3744e-02, 3.1558e-02, 1.4644e-01,\n",
       "         8.1978e-03, 8.5439e-02, 2.9618e-02, 0.0000e+00, 6.3344e-03, 3.6395e-02,\n",
       "         8.2408e-02, 1.0797e-01, 2.1553e-02, 2.6769e-02, 3.4890e-02, 7.8883e-02,\n",
       "         1.2638e-03, 9.7155e-02, 6.2550e-02, 5.6136e-03, 4.6943e-03, 5.0503e-02,\n",
       "         8.7601e-03, 2.0553e-02, 1.9239e-02, 6.7737e-03, 2.2779e-02, 1.4167e-01,\n",
       "         3.1067e-02, 1.6056e-01, 4.5809e-02, 1.5716e-01, 6.5863e-02, 7.2240e-02,\n",
       "         2.9381e-02, 5.0302e-02, 2.1542e-02, 5.3402e-02, 3.6347e-02, 1.4914e-01,\n",
       "         1.5231e-01, 2.7045e-02, 4.4851e-04, 1.4107e-01, 3.2506e-02, 1.0961e-01,\n",
       "         2.0816e-02, 4.0623e-02, 2.0464e-02, 4.9693e-03, 4.4875e-03, 1.0457e-01,\n",
       "         1.0871e-01, 5.2564e-02, 1.0539e-01, 1.5872e-01, 9.6544e-02, 1.0471e-01,\n",
       "         1.1401e-02, 0.0000e+00, 2.3032e-02, 8.9773e-02, 2.1648e-01, 1.0077e-01,\n",
       "         2.3391e-02, 3.9872e-02, 9.4218e-02, 4.2068e-02, 1.4951e-01, 1.5324e-03,\n",
       "         5.1443e-04, 1.0881e-01, 4.6071e-02, 4.5400e-02, 9.7368e-03, 1.0262e-01,\n",
       "         1.2311e-01, 9.7239e-02, 2.8512e-02, 5.2897e-02, 1.1912e-01, 6.9581e-02,\n",
       "         0.0000e+00, 1.3890e-01, 1.2033e-01, 9.3972e-02, 2.0159e-02, 1.1960e-01,\n",
       "         1.1014e-01, 6.7855e-02, 4.0544e-02, 6.3201e-02, 7.1516e-02, 7.3697e-02,\n",
       "         1.1394e-01, 7.1263e-02, 5.9578e-02, 2.5086e-02, 7.2097e-03, 1.7009e-02,\n",
       "         6.0637e-02, 1.6085e-01, 2.7582e-02, 6.7355e-02, 2.3601e-02, 9.6014e-03,\n",
       "         8.2232e-02, 1.9228e-01, 5.9071e-03, 1.3876e-02, 7.5287e-02, 1.5211e-01,\n",
       "         3.2360e-02, 3.0317e-02, 1.0115e-01, 2.0313e-02, 2.3922e-02, 4.0689e-02,\n",
       "         3.9372e-02, 3.1958e-02, 1.3504e-02, 8.2640e-02, 9.3253e-03, 8.3244e-02,\n",
       "         1.4448e-02, 1.1827e-02, 3.7859e-03, 4.7778e-02, 1.5712e-01, 5.1061e-03,\n",
       "         7.7071e-02, 1.0177e-01, 3.6395e-04, 1.3788e-01, 1.4379e-02, 3.7965e-02,\n",
       "         1.2421e-02, 3.6138e-02, 3.6845e-02, 2.0516e-02, 1.6934e-01, 3.1831e-02,\n",
       "         5.8293e-02, 1.6437e-02, 3.1727e-03, 1.8896e-01, 3.3160e-02, 1.1319e-01,\n",
       "         1.4708e-02, 8.4995e-02, 8.3707e-02, 9.2128e-03, 3.6648e-01, 5.9205e-02,\n",
       "         9.3406e-02, 3.7704e-02, 1.0460e-01, 1.0266e-03, 9.1714e-02, 5.2674e-02,\n",
       "         0.0000e+00, 3.5856e-02, 1.5795e-01, 3.0308e-03, 3.8361e-02, 1.0473e-03,\n",
       "         3.6955e-02, 3.3057e-02, 3.5091e-02, 1.2068e-02, 2.3356e-02, 2.9049e-02,\n",
       "         3.7375e-02, 0.0000e+00, 5.7337e-02, 5.8588e-02, 5.2606e-03, 1.6038e-01,\n",
       "         2.8324e-02, 1.2081e-02, 2.2433e-02, 5.8807e-02, 6.6851e-03, 2.3514e-03,\n",
       "         7.5620e-03, 9.8922e-02, 1.4798e-01, 6.8228e-02, 7.1122e-02, 2.1759e-02,\n",
       "         3.9177e-02, 1.3735e-01, 2.5131e-02, 1.0560e-03, 1.5951e-01, 9.2921e-02,\n",
       "         1.2322e-01, 3.2360e-02, 1.4727e-02, 1.3531e-01, 1.9938e-02, 5.1866e-02,\n",
       "         1.9456e-02, 2.6486e-02, 6.2792e-02, 3.8290e-02, 2.6092e-02, 9.2218e-02,\n",
       "         2.9356e-02, 1.5782e-01, 1.6660e-02, 3.4043e-02, 1.2226e-02, 1.6381e-01,\n",
       "         8.3352e-02, 3.0461e-03, 0.0000e+00, 1.7687e-01, 1.2497e-02, 2.2711e-01,\n",
       "         4.3132e-05, 3.2672e-02, 1.0803e-01, 1.1819e-01, 3.3271e-02, 2.3765e-02,\n",
       "         2.1035e-02, 6.1625e-03, 9.4476e-02, 1.9298e-01, 3.2557e-02, 2.5568e-02,\n",
       "         2.1964e-01, 9.6873e-02, 7.3681e-03, 2.6694e-02, 9.1708e-02, 8.0071e-02,\n",
       "         3.8126e-02, 5.3881e-02, 2.6222e-03, 4.7067e-02, 2.6607e-02, 5.0315e-03,\n",
       "         3.0637e-02, 1.1251e-02, 1.3502e-02, 3.4007e-02, 1.1926e-01, 2.8090e-02,\n",
       "         1.7627e-01, 0.0000e+00, 8.4699e-02, 1.2406e-01, 7.7379e-02, 6.2543e-02,\n",
       "         4.6838e-02, 1.0438e-01, 2.1487e-02, 1.4546e-02, 1.6848e-02, 2.7527e-02,\n",
       "         1.6944e-02, 6.2437e-02, 5.3666e-02, 5.8517e-02, 8.7262e-02, 1.5225e-02,\n",
       "         1.0461e-01, 1.5721e-01, 9.2153e-02, 1.2663e-01, 4.8534e-02, 7.2046e-02,\n",
       "         4.2959e-02, 1.2841e-01, 8.7880e-02, 3.3328e-02, 1.1188e-02, 9.8877e-03,\n",
       "         5.5856e-02, 3.1563e-04, 2.5425e-01, 9.4905e-02, 1.7003e-02, 7.1501e-02,\n",
       "         1.3068e-01, 1.5029e-01, 4.1122e-02, 1.5066e-01, 2.4412e-03, 1.2214e-04,\n",
       "         6.7964e-02, 7.7272e-02, 1.0203e-01, 1.1332e-02, 2.7140e-02, 1.3908e-01,\n",
       "         6.2055e-02, 2.5790e-02, 8.1947e-03, 2.6515e-02, 1.3849e-01, 2.1581e-02,\n",
       "         1.8188e-02, 1.7321e-01, 6.8444e-02, 4.8932e-04, 4.6583e-02, 2.0758e-02,\n",
       "         9.5312e-02, 2.1456e-02, 1.3294e-01, 9.6813e-02, 1.5106e-01, 1.6321e-01,\n",
       "         5.3777e-02, 2.2856e-01, 1.9983e-02, 9.9454e-02, 7.5200e-02, 1.1021e-02,\n",
       "         1.5057e-02, 8.6361e-02, 4.5178e-02, 9.1706e-02],\n",
       "        [6.0781e-02, 4.3995e-02, 1.7850e-02, 1.2277e-02, 1.3112e-01, 2.3375e-02,\n",
       "         7.2630e-02, 3.8948e-03, 6.7497e-02, 4.3381e-02, 1.9934e-02, 1.1024e-01,\n",
       "         1.0511e-01, 6.8673e-02, 1.2171e-01, 8.1965e-02, 5.6221e-02, 4.6765e-02,\n",
       "         4.1517e-02, 8.6561e-03, 2.0604e-02, 5.7480e-02, 1.4666e-02, 2.0527e-02,\n",
       "         5.8357e-02, 3.2073e-02, 0.0000e+00, 1.1934e-01, 3.4592e-02, 8.9290e-02,\n",
       "         2.2143e-02, 1.6400e-01, 2.2907e-01, 2.5854e-03, 4.7953e-03, 1.1407e-01,\n",
       "         3.3314e-02, 3.1099e-01, 1.3433e-02, 5.6675e-02, 9.0724e-03, 1.6798e-02,\n",
       "         7.1561e-02, 1.3883e-01, 7.8439e-02, 4.0836e-03, 9.4903e-02, 4.0454e-02,\n",
       "         1.2488e-02, 2.3854e-02, 5.1702e-03, 5.2116e-02, 4.0429e-02, 7.0929e-04,\n",
       "         5.2542e-02, 7.9006e-02, 3.0586e-02, 7.0496e-02, 1.9060e-01, 2.5915e-02,\n",
       "         4.9229e-02, 1.9119e-02, 1.4004e-03, 6.4681e-02, 0.0000e+00, 1.2274e-01,\n",
       "         9.8286e-02, 1.4856e-01, 7.2598e-02, 4.1103e-02, 6.2724e-02, 4.1482e-02,\n",
       "         2.0587e-02, 3.2948e-03, 9.5599e-02, 8.8163e-02, 3.7210e-02, 1.7317e-01,\n",
       "         6.8997e-03, 9.7155e-02, 1.4062e-02, 0.0000e+00, 1.9359e-02, 4.0832e-02,\n",
       "         1.2308e-01, 1.5000e-01, 1.7203e-02, 1.2495e-02, 3.7693e-02, 7.2687e-02,\n",
       "         0.0000e+00, 6.8161e-02, 8.4978e-02, 5.9122e-03, 1.3316e-02, 7.3606e-02,\n",
       "         2.3153e-02, 2.2034e-02, 5.2013e-02, 2.7295e-03, 3.4724e-02, 1.1967e-01,\n",
       "         2.3674e-02, 1.9957e-01, 6.2206e-02, 1.7291e-01, 7.6200e-02, 5.7674e-02,\n",
       "         4.6449e-02, 4.5210e-02, 2.0437e-02, 7.4001e-02, 9.8480e-03, 1.5535e-01,\n",
       "         9.0863e-02, 2.6573e-02, 1.5420e-02, 1.1791e-01, 3.3489e-02, 9.0405e-02,\n",
       "         5.0677e-02, 2.8085e-02, 1.8152e-02, 1.3919e-02, 0.0000e+00, 1.0219e-01,\n",
       "         1.2896e-01, 3.4788e-02, 1.3721e-01, 1.8109e-01, 1.1335e-01, 9.4631e-02,\n",
       "         3.7227e-03, 0.0000e+00, 2.8007e-02, 7.8847e-02, 2.5328e-01, 7.2726e-02,\n",
       "         4.4164e-02, 3.1005e-02, 5.9021e-02, 4.7936e-02, 2.1458e-01, 8.9407e-03,\n",
       "         1.4913e-03, 8.1236e-02, 6.8968e-02, 3.1110e-02, 7.8051e-03, 1.3569e-01,\n",
       "         1.1123e-01, 9.6511e-02, 2.9753e-02, 4.5495e-02, 7.0692e-02, 4.0463e-02,\n",
       "         8.9985e-03, 1.0266e-01, 1.0565e-01, 7.3588e-02, 2.0926e-02, 9.3297e-02,\n",
       "         1.1910e-01, 1.5465e-02, 4.0226e-02, 6.6187e-02, 9.0647e-02, 6.7558e-02,\n",
       "         1.0776e-01, 7.1699e-02, 5.1274e-02, 1.6660e-02, 6.5483e-03, 4.7012e-02,\n",
       "         5.9596e-02, 1.6396e-01, 3.2965e-02, 4.4383e-02, 4.0317e-02, 1.3089e-02,\n",
       "         1.2609e-01, 1.2164e-01, 5.8192e-03, 3.6525e-02, 5.0487e-02, 1.5328e-01,\n",
       "         5.5438e-02, 4.7718e-02, 9.1870e-02, 1.0339e-02, 1.6250e-02, 2.7397e-02,\n",
       "         4.6906e-02, 1.7489e-02, 1.7753e-02, 7.6345e-02, 9.6082e-03, 8.5019e-02,\n",
       "         2.7940e-02, 1.5612e-02, 1.1564e-02, 3.5656e-02, 1.4212e-01, 7.8867e-03,\n",
       "         7.1696e-02, 6.9541e-02, 1.4635e-03, 9.1855e-02, 2.4157e-02, 4.2587e-02,\n",
       "         0.0000e+00, 3.9630e-02, 5.8307e-02, 2.0263e-02, 1.4538e-01, 4.5114e-02,\n",
       "         6.3940e-02, 1.8395e-02, 4.3614e-03, 1.9304e-01, 3.3880e-02, 1.0979e-01,\n",
       "         1.5582e-02, 8.8354e-02, 1.0235e-01, 1.4089e-02, 3.5179e-01, 6.3383e-02,\n",
       "         6.7565e-02, 2.5303e-02, 1.6603e-01, 1.3686e-03, 1.1174e-01, 4.2523e-02,\n",
       "         1.0156e-02, 2.4287e-02, 1.4344e-01, 3.8132e-03, 5.7258e-02, 9.5742e-03,\n",
       "         2.6308e-02, 1.6428e-02, 4.0179e-02, 1.6308e-02, 1.2723e-02, 2.8885e-02,\n",
       "         2.0151e-02, 0.0000e+00, 6.5694e-02, 3.3031e-02, 0.0000e+00, 1.4053e-01,\n",
       "         2.1398e-02, 2.0174e-02, 2.9011e-02, 4.8181e-02, 4.2179e-02, 6.6914e-03,\n",
       "         9.1039e-03, 1.0306e-01, 1.1391e-01, 8.1279e-02, 8.4529e-02, 1.1981e-02,\n",
       "         3.8551e-02, 1.5812e-01, 2.4269e-02, 1.5731e-03, 1.2621e-01, 9.6866e-02,\n",
       "         1.0011e-01, 3.8569e-02, 4.7917e-03, 1.3243e-01, 5.0140e-02, 6.8925e-02,\n",
       "         2.1303e-02, 3.1184e-02, 7.2065e-02, 4.9699e-02, 6.9393e-02, 7.8982e-02,\n",
       "         2.7731e-02, 8.3664e-02, 2.0377e-02, 2.3751e-02, 9.4366e-03, 1.6119e-01,\n",
       "         6.4568e-02, 4.3047e-03, 0.0000e+00, 1.5079e-01, 1.1607e-02, 2.3743e-01,\n",
       "         3.7011e-03, 5.1314e-02, 1.0997e-01, 1.2761e-01, 3.7905e-02, 2.2767e-02,\n",
       "         1.8856e-02, 1.4160e-02, 7.7042e-02, 1.8794e-01, 4.4702e-02, 1.4353e-02,\n",
       "         2.4160e-01, 9.6230e-02, 1.5003e-02, 3.6532e-02, 4.8510e-02, 4.8642e-02,\n",
       "         5.3962e-02, 6.0722e-02, 2.8896e-03, 4.0297e-02, 2.8453e-02, 2.6530e-02,\n",
       "         2.1176e-02, 1.9942e-02, 1.5043e-02, 4.2129e-02, 1.4216e-01, 3.4117e-02,\n",
       "         1.9198e-01, 0.0000e+00, 1.1070e-01, 1.3468e-01, 9.9307e-02, 9.3965e-02,\n",
       "         5.0396e-02, 1.2620e-01, 2.5233e-02, 1.0954e-03, 2.3530e-02, 1.0971e-02,\n",
       "         2.6433e-02, 3.9631e-02, 8.7475e-02, 4.3645e-02, 8.6861e-02, 9.4905e-03,\n",
       "         1.0474e-01, 1.3741e-01, 7.4353e-02, 1.3845e-01, 3.5816e-02, 9.4228e-02,\n",
       "         3.1151e-02, 1.5484e-01, 8.8665e-02, 5.1201e-02, 2.0814e-02, 9.7077e-03,\n",
       "         3.6350e-02, 3.1666e-03, 3.5300e-01, 8.5694e-02, 1.1756e-02, 6.0241e-02,\n",
       "         1.3103e-01, 1.9798e-01, 1.7919e-02, 1.3036e-01, 7.9096e-03, 6.7023e-03,\n",
       "         7.2303e-02, 1.0938e-01, 1.0662e-01, 1.0375e-02, 1.6307e-02, 1.2554e-01,\n",
       "         5.7689e-02, 5.9418e-02, 1.5408e-03, 2.6088e-02, 1.3370e-01, 2.4199e-02,\n",
       "         2.1004e-02, 2.0645e-01, 4.9463e-02, 4.3441e-03, 7.0800e-02, 7.2435e-03,\n",
       "         8.5597e-02, 4.0424e-02, 7.4476e-02, 6.8881e-02, 1.8007e-01, 1.0534e-01,\n",
       "         7.7103e-02, 2.7922e-01, 2.0160e-02, 1.0988e-01, 6.0610e-02, 5.7651e-03,\n",
       "         2.0723e-02, 1.4254e-01, 9.8874e-02, 9.6495e-02]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_pt = DocEncoder()\n",
    "dim1 = 30\n",
    "dim2 = 300\n",
    "x = torch.randn((2, dim1, dim2))\n",
    "\n",
    "d_pt.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "news_vecs_input (None, 50, 400)\n",
      "attention_user_vecs2 (None, 50, 400)\n",
      "tail20_user_vec1 (None, 20, 400)\n",
      "dropout_user_vecs2 (None, 50, 400)\n",
      "gru_user_vec1 (None, 400)\n",
      "pooling_user_vec2 (None, 400)\n",
      "reshape_2 (None, 1, 400)\n",
      "reshape_1 (None, 1, 400)\n",
      "concatenate_1 (None, 2, 400)\n",
      "vec_pool (None, 400)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_user_encoder():\n",
    "    news_vecs_input = Input(shape=(50,400), dtype='float32', name='news_vecs_input')\n",
    "    \n",
    "    news_vecs = Dropout(0.2, name='news_vecs')(news_vecs_input)\n",
    "    gru_input = keras.layers.Lambda(lambda x:x[:,-15:,:])(news_vecs)\n",
    "    vec1 = GRU(400)(gru_input)\n",
    "    vecs2 = Attention(20,20, name='vecs2')([news_vecs]*3)\n",
    "    vec2 = tfAttentivePooling(50,400)(vecs2)\n",
    "\n",
    "    user_vecs2 = Attention(20,20, name='attention_user_vecs2')([news_vecs_input]*3)\n",
    "    user_vecs2 = Dropout(0.2, name='dropout_user_vecs2')(user_vecs2)\n",
    "    user_vec2 = tfAttentivePooling(50,400, name='pooling_user_vec2')(user_vecs2)\n",
    "    user_vec2 = keras.layers.Reshape((1,400))(user_vec2)\n",
    "        \n",
    "    user_vecs1 = Lambda(lambda x:x[:,-20:,:], name='tail20_user_vec1')(news_vecs_input)\n",
    "    user_vec1 = GRU(400, name=\"gru_user_vec1\")(user_vecs1)\n",
    "    user_vec1 = keras.layers.Reshape((1,400))(user_vec1)\n",
    "\n",
    "    user_vecs = keras.layers.Concatenate(axis=-2)([user_vec1,user_vec2])\n",
    "    vec = tfAttentivePooling(2,400, name='vec_pool')(user_vecs)\n",
    "        \n",
    "    sentEncodert = Model(news_vecs_input, vec, name='user_encoder')\n",
    "    return sentEncodert\n",
    "\n",
    "u = get_user_encoder()\n",
    "print_model(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VecTail(nn.Module):\n",
    "    def __init__(self, n):\n",
    "        super(VecTail, self).__init__()\n",
    "        self.n = n\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:,-self.n:,:]\n",
    "\n",
    "class UserEncoder(nn.Module):\n",
    "    def __init__(self):        \n",
    "        super(UserEncoder,self).__init__()\n",
    "        # news_vecs_input = Input(shape=(50,400), dtype='float32')\n",
    "        #self.dropout1 = nn.Dropout(0.2)\n",
    "        #self.tail = VecTail(15)\n",
    "        #self.gru = nn.GRU(400, 400)\n",
    "        #self.attention = nn.MultiheadAttention(400, 20)\n",
    "        #self.pool = AttentivePooling(50, 400)\n",
    "        self.attention2 = nn.MultiheadAttention(400, 20)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.pool2 = AttentivePooling(50, 400)\n",
    "        self.tail2 = VecTail(20)\n",
    "        self.gru2 = nn.GRU(400,400, batch_first=True)\n",
    "        self.pool3 = AttentivePooling(2, 400)\n",
    "\n",
    "    def forward(self, news_vecs_input):    \n",
    "        #news_vecs =self.dropout1(news_vecs_input)\n",
    "        #gru_input = self.tail(news_vecs)\n",
    "        #vec1 = self.gru(gru_input)\n",
    "        #vecs2 = self.attention(*[news_vecs]*3)\n",
    "        #vec2 = self.pool(vecs2)\n",
    "        print('news_vecs_input', news_vecs_input.shape)\n",
    "        user_vecs2, _u_weights = self.attention2(*[news_vecs_input]*3)\n",
    "        user_vecs2 = self.dropout2(user_vecs2)\n",
    "        user_vec2 = self.pool2(user_vecs2)\n",
    "        print('pool2_user_vec2', user_vec2.shape)\n",
    "        #user_vec2 = keras.layers.Reshape((1,400))(user_vec2)\n",
    "        #user_vec2 = user_vec2.unsqueeze(1)\n",
    "\n",
    "        user_vecs1 = self.tail2(news_vecs_input)\n",
    "        print('tail2_user_vecs1', user_vecs1.shape)\n",
    "        user_vec1, _u_hidden = self.gru2(user_vecs1)\n",
    "        print('gru2_user_vec1', user_vec1.shape)\n",
    "        user_vec1 = user_vec1[:, -1, :]\n",
    "        #user_vec1 = keras.layers.Reshape((1,400))(user_vec1)\n",
    "        #user_vec1 = user_vec1.unsqueeze(1)\n",
    "        \n",
    "        user_vecs = torch.stack([user_vec1, user_vec2], dim=1) #keras.layers.Concatenate(axis=-2)([user_vec1,user_vec2])\n",
    "        print(user_vecs.shape)\n",
    "        vec = self.pool3(user_vecs)\n",
    "        print(vec.shape)\n",
    "        return vec\n",
    "        \n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "news_vecs_input torch.Size([2, 50, 400])\n",
      "pool2_user_vec2 torch.Size([2, 400])\n",
      "tail2_user_vecs1 torch.Size([2, 20, 400])\n",
      "gru2_user_vec1 torch.Size([2, 20, 400])\n",
      "torch.Size([2, 2, 400])\n",
      "torch.Size([2, 400])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.6806e-02,  1.3265e-02, -1.6694e-01,  2.1280e-02,  1.8720e-01,\n",
       "         -7.0635e-03, -3.2158e-01,  1.3890e-02, -7.7261e-02, -7.7432e-03,\n",
       "         -9.5752e-02, -8.2957e-02,  2.1183e-01, -2.6020e-02, -7.8267e-03,\n",
       "         -5.9289e-02,  1.1708e-02,  1.6052e-02, -3.6814e-01,  6.8838e-02,\n",
       "          6.1980e-03,  1.1181e-01,  2.0714e-01,  1.4299e-02,  2.5372e-02,\n",
       "          2.4576e-03, -2.4605e-01,  1.6293e-01, -1.0431e-02,  0.0000e+00,\n",
       "         -1.6275e-01,  2.9142e-02,  2.1689e-02,  1.5156e-01,  2.6457e-01,\n",
       "         -7.2167e-02,  5.7367e-02, -1.2071e-01, -4.0362e-02, -1.9674e-01,\n",
       "          6.6915e-02, -2.7688e-01,  2.2345e-01, -9.4654e-02, -3.6159e-01,\n",
       "          1.3844e-02,  6.0253e-02, -2.1889e-01,  2.8887e-01, -1.4123e-01,\n",
       "          3.2666e-01, -3.0967e-02,  1.6158e-01, -6.5765e-03,  6.3870e-03,\n",
       "          1.4175e-01,  1.9355e-01,  1.1138e-01, -1.9907e-02,  1.3860e-01,\n",
       "         -5.9715e-02,  3.5732e-01, -3.3400e-02,  1.0295e-01, -1.7454e-02,\n",
       "         -3.1576e-01, -6.5066e-02, -1.0376e-01,  7.0093e-02,  2.8640e-02,\n",
       "          2.7713e-01, -3.5875e-02, -2.0121e-01,  2.4349e-01,  1.4930e-01,\n",
       "         -5.9020e-02, -3.7008e-02, -1.4405e-01, -1.0294e-02, -3.6096e-01,\n",
       "          0.0000e+00,  2.1990e-01, -6.3234e-02,  3.2526e-01,  8.2610e-03,\n",
       "          0.0000e+00,  3.9020e-02,  2.6469e-01,  2.2043e-01,  3.4466e-02,\n",
       "         -2.1000e-01,  7.8322e-02, -5.3702e-03, -2.5337e-01,  1.3514e-02,\n",
       "          1.3072e-01, -1.2251e-01,  3.4776e-01, -2.7431e-01,  2.4265e-01,\n",
       "         -1.7069e-02, -4.2465e-02,  1.4822e-01,  1.6284e-01, -3.2915e-01,\n",
       "         -2.3986e-01,  2.6724e-01, -1.3268e-01,  1.8758e-02,  4.9950e-02,\n",
       "         -3.7385e-02, -1.0792e-01,  3.5782e-01, -1.1120e-01, -2.2874e-02,\n",
       "         -3.2751e-02, -1.9566e-01,  4.8603e-02, -1.0801e-01, -1.8107e-02,\n",
       "          1.4416e-01,  1.8845e-01,  9.7894e-02,  3.0544e-01,  1.6369e-02,\n",
       "         -3.2342e-01,  6.1311e-02, -5.4489e-03, -2.4319e-02,  2.8879e-01,\n",
       "         -1.6376e-01, -2.2641e-01, -2.4674e-01,  9.4762e-02, -2.5375e-01,\n",
       "          3.8728e-01, -1.4195e-01,  0.0000e+00, -1.2992e-01, -5.4698e-02,\n",
       "          1.3144e-01,  0.0000e+00, -6.1469e-02,  1.4938e-02, -8.0414e-03,\n",
       "          6.9407e-02, -3.5203e-01, -1.2764e-01, -4.4120e-02,  1.4038e-01,\n",
       "         -1.6413e-02,  1.2182e-01,  2.6583e-02,  8.0104e-02,  0.0000e+00,\n",
       "         -2.3201e-02,  2.6189e-02, -7.9797e-02,  1.1675e-01, -9.5032e-02,\n",
       "         -5.1343e-02, -6.7484e-03, -4.4783e-02,  7.4707e-02,  6.2888e-02,\n",
       "          1.3382e-01,  4.1177e-02,  1.5993e-02, -1.0128e-01, -8.5760e-03,\n",
       "         -1.3930e-02,  5.9939e-02,  0.0000e+00,  3.1780e-02, -2.4666e-01,\n",
       "          2.1914e-01, -6.1536e-02, -2.0939e-01, -3.6457e-01,  3.3799e-02,\n",
       "         -1.4568e-02,  1.1594e-01,  2.1689e-01,  8.4606e-02,  3.2559e-04,\n",
       "          1.7508e-01,  3.3480e-02, -6.6756e-03,  1.1059e-01, -1.5069e-02,\n",
       "         -7.4737e-02,  1.3719e-01,  7.3988e-04,  0.0000e+00, -8.5695e-04,\n",
       "         -3.0693e-01,  0.0000e+00,  1.4924e-01, -2.4737e-01,  2.5265e-01,\n",
       "         -1.4501e-01,  2.0072e-01, -1.4618e-01, -2.5421e-01,  0.0000e+00,\n",
       "          8.9321e-02,  1.0788e-01, -4.1950e-02, -2.5655e-03,  1.6526e-02,\n",
       "          1.5947e-01, -1.1462e-02, -2.7890e-01,  1.1122e-01, -9.3640e-02,\n",
       "          8.1196e-02, -2.4574e-01, -1.1454e-01,  3.5373e-01, -2.9640e-01,\n",
       "         -2.8384e-01,  1.1522e-01, -5.0831e-02,  7.8416e-02, -8.3489e-02,\n",
       "         -3.1359e-01,  1.0416e-01,  1.9949e-01,  0.0000e+00,  1.4022e-01,\n",
       "         -1.1992e-01,  2.2365e-01, -3.7102e-03,  1.1258e-01,  2.6556e-01,\n",
       "          1.4289e-01, -1.1633e-01,  4.0920e-02, -1.9332e-01,  1.6550e-01,\n",
       "          2.3810e-02, -6.6022e-02, -9.3577e-02,  2.9055e-01, -4.2151e-02,\n",
       "         -3.0343e-02, -3.0317e-01, -2.4307e-01, -5.9077e-03,  9.4389e-02,\n",
       "         -1.1227e-01, -2.2955e-01,  1.5324e-01,  9.1572e-02,  2.0716e-01,\n",
       "         -1.4578e-01,  8.9485e-02, -1.1006e-02, -9.2052e-02, -3.7228e-02,\n",
       "          9.7629e-02, -2.7140e-01, -1.5237e-01, -2.8028e-01,  1.6022e-01,\n",
       "         -5.7626e-02,  2.1806e-02,  2.7112e-01,  8.5734e-02,  1.1726e-01,\n",
       "          1.0167e-02,  2.1460e-01,  1.8850e-01, -1.7459e-02, -2.4747e-01,\n",
       "          1.9319e-02, -1.3084e-01, -2.9728e-01, -1.4194e-01,  1.8833e-01,\n",
       "         -2.4493e-02,  6.0507e-02, -8.5728e-02, -1.6426e-02, -1.6944e-02,\n",
       "          2.4304e-02,  4.5151e-01,  1.2269e-01,  1.2160e-01,  3.0214e-02,\n",
       "         -1.6166e-02,  1.3351e-01, -1.5218e-01,  3.3504e-02,  2.4442e-01,\n",
       "          2.4909e-02, -1.7708e-01,  3.0693e-02,  1.4610e-02, -5.5997e-02,\n",
       "          3.6288e-02, -3.5985e-02, -1.1349e-01, -3.8496e-02,  6.2196e-02,\n",
       "         -7.2427e-02, -5.9205e-02,  1.1321e-02, -1.0065e-01,  1.2032e-01,\n",
       "          1.8465e-01,  3.2319e-01,  0.0000e+00,  3.1134e-02,  1.9783e-02,\n",
       "         -2.6035e-01, -1.1148e-01,  1.1640e-01,  1.3721e-01, -3.2547e-02,\n",
       "          2.1535e-01, -1.2835e-01,  1.7818e-03, -5.8850e-02,  1.4837e-03,\n",
       "         -2.1052e-01, -1.2002e-02,  1.5482e-01,  2.9714e-02, -1.5243e-02,\n",
       "          2.2445e-01,  3.9577e-03, -7.6653e-02, -1.7799e-01,  2.6226e-02,\n",
       "          7.2467e-02,  3.6977e-02,  8.0168e-03, -1.4496e-01,  5.1375e-03,\n",
       "          1.2004e-01, -1.1289e-01, -8.8991e-03, -1.5265e-01, -1.9001e-02,\n",
       "         -9.6035e-02,  4.9735e-02,  0.0000e+00,  2.4422e-01, -1.6389e-02,\n",
       "         -2.1611e-02, -1.8423e-01, -1.0483e-01, -4.0234e-02, -6.8061e-02,\n",
       "          2.6169e-01,  1.8448e-01,  1.2779e-01,  9.6296e-04, -5.8977e-02,\n",
       "         -3.3292e-01,  1.2497e-01, -7.8267e-02,  7.4574e-02,  1.8561e-02,\n",
       "          1.2914e-01,  0.0000e+00, -4.9897e-02,  3.4454e-02, -3.8314e-02,\n",
       "          1.7386e-01,  2.6341e-01,  1.6551e-02,  3.1489e-03, -1.4982e-01,\n",
       "         -9.7699e-02,  2.4127e-01,  8.8715e-02,  2.5685e-02, -2.2033e-01,\n",
       "          0.0000e+00, -1.0619e-01, -9.7724e-02, -1.2105e-01,  2.6986e-01,\n",
       "         -9.0768e-02, -7.9283e-02,  3.9609e-02, -1.5940e-01, -1.5982e-01,\n",
       "          1.2828e-01,  0.0000e+00,  1.1501e-01,  8.9100e-02,  2.2519e-02,\n",
       "          2.4406e-01, -1.6865e-01,  0.0000e+00, -1.8476e-01,  2.0176e-01],\n",
       "        [ 2.9533e-01,  1.4496e-01, -5.9078e-02, -5.8922e-02, -2.2364e-01,\n",
       "         -2.0491e-01,  4.7044e-02, -3.6517e-01,  1.3014e-01, -9.3963e-02,\n",
       "          9.1327e-02, -1.2814e-01,  2.4704e-03, -1.5865e-01,  1.5384e-02,\n",
       "         -1.1854e-01,  1.6991e-01,  1.3385e-01, -2.4316e-02,  2.9016e-02,\n",
       "          5.2078e-02, -1.4578e-01, -2.1736e-01, -4.4261e-02, -2.1671e-02,\n",
       "         -3.0251e-02,  0.0000e+00,  1.1291e-01,  1.9200e-01,  1.8577e-02,\n",
       "         -1.9012e-01,  5.8768e-02,  6.1619e-03, -2.0775e-01,  9.3589e-02,\n",
       "          3.2713e-01,  1.5475e-02,  4.0740e-02, -1.6197e-02, -4.4018e-02,\n",
       "          7.5124e-02,  1.3448e-02, -2.9732e-01,  1.3616e-01,  1.6810e-01,\n",
       "          1.2461e-01,  6.4536e-02, -5.7918e-03,  5.3991e-02,  2.8320e-01,\n",
       "         -4.1047e-03, -1.0029e-01,  1.3275e-01,  2.7330e-03, -7.2918e-02,\n",
       "          2.7051e-01,  1.1309e-01,  9.1859e-02, -2.2704e-01, -9.6315e-03,\n",
       "          3.9094e-02,  1.7933e-04, -1.6338e-01, -1.8203e-02,  1.3452e-01,\n",
       "          2.6891e-02, -2.6194e-01,  4.3205e-02,  1.4093e-01, -1.9153e-01,\n",
       "         -1.7853e-01, -5.2583e-03, -3.5618e-01, -5.2689e-02,  1.0855e-01,\n",
       "          2.3118e-01, -2.3044e-01, -2.9120e-01,  0.0000e+00,  7.5497e-02,\n",
       "         -3.0014e-01,  3.3339e-01, -9.7999e-02,  0.0000e+00, -2.2215e-01,\n",
       "          4.2747e-02,  3.7295e-01,  2.2702e-02,  1.0312e-01, -1.3576e-01,\n",
       "          1.0136e-01,  2.2231e-02, -2.0828e-01,  6.2225e-02, -1.4372e-01,\n",
       "         -2.4487e-01,  6.5094e-02, -3.9686e-02, -5.1076e-02,  1.3242e-01,\n",
       "          2.1252e-01,  1.7520e-01, -2.2650e-01,  1.3858e-01,  5.2804e-02,\n",
       "         -1.9236e-02,  1.8191e-01,  1.4340e-02, -3.3120e-02, -3.2983e-02,\n",
       "          4.6676e-02,  9.7485e-02,  1.2216e-01,  2.4101e-02, -3.9282e-03,\n",
       "          2.7728e-01,  7.6620e-02,  2.7617e-01,  6.2012e-02, -1.9822e-01,\n",
       "          3.7818e-01, -1.1622e-02,  3.3282e-03, -3.3432e-01,  2.7409e-01,\n",
       "         -5.2732e-02, -7.8562e-02, -2.5526e-01,  0.0000e+00,  2.6967e-01,\n",
       "          2.2109e-01,  1.8823e-01, -3.2280e-01, -1.0935e-03, -6.8670e-03,\n",
       "          1.9999e-01,  1.4938e-01,  1.9302e-02,  4.8527e-02,  4.6585e-02,\n",
       "          2.2070e-01, -9.5156e-02,  1.7790e-01,  1.1828e-01, -1.8619e-01,\n",
       "          1.1926e-01,  0.0000e+00, -4.6648e-02,  2.0366e-02,  1.5489e-01,\n",
       "         -6.0940e-02, -2.3012e-01,  5.1564e-02,  0.0000e+00, -1.4344e-01,\n",
       "         -1.0958e-01,  1.4179e-01,  7.0957e-02,  7.5317e-02, -2.5361e-02,\n",
       "         -6.3668e-02, -1.0235e-02,  6.6675e-02,  1.2411e-01, -7.0844e-02,\n",
       "         -8.1981e-02,  1.2411e-02,  2.5400e-01,  2.7357e-01, -2.7619e-01,\n",
       "          2.5509e-02,  1.2088e-02,  2.4428e-02,  5.5805e-02, -3.7940e-02,\n",
       "         -4.7601e-02, -2.2140e-01,  2.6277e-03,  2.9397e-03,  1.6066e-01,\n",
       "         -2.4241e-02,  0.0000e+00, -4.7723e-02,  6.6154e-03,  3.8491e-02,\n",
       "         -1.3468e-01,  2.3655e-01, -1.7332e-01, -3.5476e-02,  0.0000e+00,\n",
       "          3.5299e-01, -6.8899e-03, -2.2913e-02,  3.4996e-01, -2.2235e-01,\n",
       "          1.0126e-01,  5.0527e-02,  8.7777e-02, -7.0745e-02,  1.3739e-01,\n",
       "          2.5827e-02,  1.3899e-01, -2.9497e-02, -1.1344e-02, -4.4528e-02,\n",
       "          0.0000e+00, -5.2190e-02, -9.4848e-02, -2.8644e-01,  1.0359e-01,\n",
       "         -1.1961e-03,  1.0849e-02, -2.0437e-02, -6.9408e-02, -4.6589e-02,\n",
       "         -2.7253e-01,  2.1588e-02,  1.2252e-01, -8.7698e-02, -1.4385e-02,\n",
       "          1.1266e-01,  3.3426e-02, -4.4843e-02,  1.7504e-01,  2.8716e-01,\n",
       "          3.6420e-02, -1.1115e-01, -2.1323e-02,  8.4870e-02,  1.2632e-01,\n",
       "         -7.3689e-04, -1.3390e-01, -5.7637e-02, -2.8369e-01,  1.4748e-01,\n",
       "         -5.6464e-02, -1.5340e-01,  6.5253e-02, -1.2756e-01, -2.5848e-01,\n",
       "          7.1688e-02,  1.2257e-01,  2.2762e-01, -1.3298e-01,  1.4645e-02,\n",
       "          2.6550e-01, -4.4236e-02,  1.5547e-01, -3.5051e-01, -2.9589e-01,\n",
       "         -1.2651e-01, -2.7230e-01, -7.5814e-02,  2.3891e-01, -3.0769e-02,\n",
       "         -5.0653e-02, -9.0811e-03, -2.5394e-01, -2.5859e-01, -1.5340e-01,\n",
       "          5.7683e-01,  2.2801e-01, -2.4280e-01,  5.4277e-02, -2.6863e-01,\n",
       "         -1.8415e-03, -1.1421e-01, -2.2970e-01,  1.9826e-02,  5.0813e-02,\n",
       "         -5.9106e-02,  1.7289e-01,  1.8665e-01, -2.4067e-01,  3.6175e-01,\n",
       "          6.1503e-04,  0.0000e+00, -2.4333e-01, -4.1948e-02, -2.3075e-01,\n",
       "         -2.9430e-01,  1.0516e-01, -1.8461e-01,  3.0533e-02, -2.5927e-02,\n",
       "         -1.6182e-02,  1.6933e-01,  6.5662e-02,  3.1249e-01, -1.6188e-01,\n",
       "         -1.9301e-01, -2.5206e-01, -1.7534e-03,  4.5178e-02, -1.8707e-02,\n",
       "         -2.3758e-01, -1.4998e-01,  1.7125e-01,  2.0576e-01,  1.0477e-01,\n",
       "          2.4022e-01, -5.3782e-03,  4.1793e-02, -1.3793e-01,  1.7233e-01,\n",
       "          7.2044e-02, -8.2625e-02,  3.4142e-01, -7.2302e-02, -9.1862e-02,\n",
       "         -7.5852e-02,  3.6204e-02,  3.9628e-02,  9.5026e-02,  0.0000e+00,\n",
       "          2.9601e-01, -2.0661e-02, -1.7278e-01,  5.1531e-02, -7.7663e-02,\n",
       "         -5.0547e-02,  2.5654e-01, -1.8452e-02, -2.5097e-01, -2.1046e-01,\n",
       "          1.1978e-02,  5.4433e-02, -4.2193e-01, -3.0886e-02, -8.3054e-03,\n",
       "         -1.7121e-01,  3.9018e-02,  2.8033e-02,  4.7177e-02, -9.6899e-02,\n",
       "          1.3943e-01,  5.2490e-02,  2.0796e-01, -7.5049e-03, -9.8621e-02,\n",
       "         -5.7265e-02,  1.8540e-01,  2.2883e-01, -1.5010e-02, -3.3692e-01,\n",
       "         -1.0718e-01,  2.3699e-02,  4.2989e-02, -1.1323e-01,  1.3389e-02,\n",
       "         -7.0559e-02,  2.3210e-02, -3.9656e-01, -3.2041e-01,  1.5383e-01,\n",
       "          6.6586e-02, -9.4214e-02,  5.1427e-02,  1.5871e-01,  8.8968e-03,\n",
       "         -9.7790e-02, -1.8284e-02,  6.7984e-02, -9.0308e-02, -1.1210e-03,\n",
       "          1.4962e-02, -2.5746e-01, -4.7977e-03, -7.6773e-02, -3.5264e-01,\n",
       "          1.2923e-02, -5.1823e-03, -3.2271e-02,  4.0205e-02,  2.7921e-01,\n",
       "          1.6627e-01,  8.0465e-02,  8.8222e-02, -3.8885e-02, -1.2132e-01,\n",
       "          1.9378e-02,  1.2322e-01,  9.3199e-04,  1.3098e-01, -1.6253e-02,\n",
       "         -5.9586e-02, -2.0485e-01, -7.6768e-02,  1.4414e-02,  1.0639e-01,\n",
       "          6.4957e-03, -4.8257e-02,  3.3847e-01, -2.8987e-01,  1.9054e-01,\n",
       "          8.4008e-02,  8.8869e-02,  2.1619e-02, -8.8916e-02,  3.8984e-02]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_pt = UserEncoder()\n",
    "dim1 = 50\n",
    "dim2 = 400\n",
    "x = torch.randn((2, dim1, dim2))\n",
    "\n",
    "u_pt.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-15 21:29:11.323378: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
      "2022-05-15 21:29:11.323436: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-05-15 21:29:11.323441: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
      "2022-05-15 21:29:11.323445: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
      "2022-05-15 21:29:11.323532: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13983 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0001:00:00.0, compute capability: 7.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model\n",
      "input_13 (None, 50, 30)\n",
      "embedding_1 output0 (None, 50, 30, 300)\n",
      "input_14 (None, 5, 30)\n",
      "td_clicks (None, 50, 400)\n",
      "user_encoder (None, 400)\n",
      "td_can (None, 5, 400)\n",
      "dot_11 (None, 5)\n",
      "recommend (None, 5)\n",
      "******\n",
      "doc\n",
      "input_8 (None, 30, 300)\n",
      "dropout_9 (None, 30, 300)\n",
      "conv1d_2 (None, 28, 400)\n",
      "dropout_10 (None, 28, 400)\n",
      "attention_2 (None, 28, 400)\n",
      "activation_7 (None, 28, 400)\n",
      "dropout_11 (None, 28, 400)\n",
      "model_6 (None, 400)\n",
      "******\n",
      "user\n",
      "news_vecs_input (None, 50, 400)\n",
      "attention_user_vecs2 (None, 50, 400)\n",
      "tail20_user_vec1 (None, 20, 400)\n",
      "dropout_user_vecs2 (None, 50, 400)\n",
      "gru_user_vec1 (None, 400)\n",
      "pooling_user_vec2 (None, 400)\n",
      "reshape_4 (None, 1, 400)\n",
      "reshape_3 (None, 1, 400)\n",
      "concatenate_2 (None, 2, 400)\n",
      "vec_pool (None, 400)\n",
      "******\n",
      "news\n",
      "input_15 (None, 30)\n",
      "embedding_1 output0 (None, 50, 30, 300)\n",
      "model_7 (None, 400)\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import SGD\n",
    "from keras.layers import TimeDistributed as tfTimeDistributed\n",
    "npratio = 4\n",
    "def get_model(lr,delta,title_word_embedding_matrix,optimizer_name='sgd'):\n",
    "    doc_encoder = get_doc_encoder()\n",
    "    user_encoder = get_user_encoder()\n",
    "    \n",
    "    title_word_embedding_layer = Embedding(title_word_embedding_matrix.shape[0], 300, weights=[title_word_embedding_matrix],trainable=False)\n",
    "    \n",
    "    click_title = Input(shape=(50,30),dtype='int32')\n",
    "    can_title = Input(shape=(1+npratio,30),dtype='int32')\n",
    "    \n",
    "    click_word_vecs = title_word_embedding_layer(click_title)\n",
    "    can_word_vecs = title_word_embedding_layer(can_title)\n",
    "    \n",
    "    click_vecs = tfTimeDistributed(doc_encoder, name='td_clicks')(click_word_vecs)\n",
    "    can_vecs = tfTimeDistributed(doc_encoder, name='td_can')(can_word_vecs)\n",
    "    \n",
    "    user_vec = user_encoder(click_vecs)\n",
    "    \n",
    "    scores = keras.layers.Dot(axes=-1)([user_vec,can_vecs]) #(batch_size,1+1,) \n",
    "    logits = keras.layers.Activation(keras.activations.softmax, name='recommend')(scores)     \n",
    "    \n",
    "    optimizer = None\n",
    "    if optimizer_name == 'sgd':\n",
    "        optimizer = SGD(lr=lr, clipvalue=delta)\n",
    "    elif optimizer_name == 'adam':\n",
    "        optimizer = Adam(lr=lr, clipvalue=delta)\n",
    "    assert optimizer\n",
    "\n",
    "    model = Model([can_title,click_title],logits, name='model') # max prob_click_positive\n",
    "    model.compile(loss=['categorical_crossentropy'],\n",
    "                  optimizer=optimizer, \n",
    "                  metrics=['acc'])\n",
    "    \n",
    "    news_input = Input(shape=(30,),dtype='int32')\n",
    "    news_word_vecs = title_word_embedding_layer(news_input)\n",
    "    news_vec = doc_encoder(news_word_vecs)\n",
    "    news_encoder = Model(news_input,news_vec, name='news_encoder')\n",
    "    \n",
    "    return model, doc_encoder, user_encoder, news_encoder\n",
    "\n",
    "mtx = np.random.uniform(size=(10,300)).astype('float32')\n",
    "#mtx_tf = tf.convert_to_tensor(mtx)\n",
    "#print(mtx_tf.shape[0])\n",
    "model, doc_encoder, user_encoder, news_encoder = get_model(0.1, 0.0, mtx)\n",
    "\n",
    "print('model')\n",
    "print_model(model)\n",
    "print('******')\n",
    "print('doc')\n",
    "print_model(doc_encoder)\n",
    "print('******')\n",
    "print('user')\n",
    "print_model(user_encoder)\n",
    "print('******')\n",
    "print('news')\n",
    "print_model(news_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# todo: verify\n",
    "class TimeDistributed(nn.Module):    \n",
    "    def __init__(self, module): #, batch_first=False):\n",
    "        super(TimeDistributed, self).__init__()\n",
    "        self.module = module\n",
    "        # self.batch_first = batch_first\n",
    "\n",
    "    def forward(self, x):\n",
    "        print('TimeDist_x',x.size())\n",
    "        if len(x.size()) <= 2:\n",
    "            return self.module(x)\n",
    "\n",
    "        output = torch.tensor([])\n",
    "        for i in range(x.size(1)):\n",
    "          output_t = self.module(x[:, i, :, :])\n",
    "          output_t  = output_t.unsqueeze(1)\n",
    "          output = torch.cat((output, output_t ), 1)\n",
    "        print('TimeDist_output', output.size())\n",
    "        return output\n",
    "        # # Squash samples and timesteps into a single axis\n",
    "        # x_reshape = x.contiguous().view(x.size(0), -1, x.size(-1))  # (samples * timesteps, input_size)\n",
    "        #print('TimeDist_x_reshape',x_reshape.shape)\n",
    "        # y = self.module(x_reshape)\n",
    "        # print('TimeDist_y', y.shape)\n",
    "        # # We have to reshape Y\n",
    "        # if self.batch_first:\n",
    "        #     y = y.contiguous().view(x.size(0), -1, y.size(-1))  # (samples, timesteps, output_size)\n",
    "        # else:\n",
    "        #    y = y.view(-1, x.size(1), y.size(-1))  # (timesteps, samples, output_size)\n",
    "        # print('TimeDist_y_reshape',y.size())\n",
    "        #return y\n",
    "\n",
    "class FedNewsRec(nn.Module):\n",
    "    def __init__(self, title_word_embedding_matrix):\n",
    "        super(FedNewsRec, self).__init__()\n",
    "        self.doc_encoder = DocEncoder() \n",
    "        self.user_encoder = UserEncoder()\n",
    "        # TODO: should this embedding matrix be frozen?\n",
    "        self.title_word_embedding_layer = nn.Embedding.from_pretrained(torch.tensor(title_word_embedding_matrix), freeze=False)\n",
    "    \n",
    "        # click_title = Input(shape=(50,30),dtype='int32')\n",
    "        # can_title = Input(shape=(1+npratio,30),dtype='int32')\n",
    "    \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.click_td = TimeDistributed(self.doc_encoder) #, batch_first=True)\n",
    "        self.can_td = TimeDistributed(self.doc_encoder) #, batch_first=True)\n",
    "        \n",
    "    def forward(self, click_title, can_title, news_input):\n",
    "        \n",
    "        click_word_vecs = self.title_word_embedding_layer(click_title)\n",
    "        print('click',click_word_vecs.shape)\n",
    "        can_word_vecs = self.title_word_embedding_layer(can_title)\n",
    "        print('can', can_word_vecs.shape)\n",
    "        click_vecs = self.click_td(click_word_vecs)\n",
    "        print('click_vecs (None, 50, 400)', click_vecs.shape)\n",
    "        can_vecs = self.can_td(can_word_vecs)\n",
    "        print('can_vecs (None, 5, 400)', can_vecs.shape)\n",
    "    \n",
    "        user_vec = self.user_encoder(click_vecs)        \n",
    "        print('user_vec (None, 400)', user_vec.shape)\n",
    "        # TODO verify\n",
    "        scores = torch.einsum('ijk,ik->ij',  can_vecs, user_vec)\n",
    "        print('scores  (None, 5)', scores.shape)\n",
    "        logits = self.softmax(scores)     \n",
    "        print('logits  (None, 5)', logits.shape)\n",
    "        \n",
    "        news_word_vecs = self.title_word_embedding_layer(news_input)\n",
    "        news_vec = self.doc_encoder(news_word_vecs)\n",
    "        \n",
    "        print('user_vec', user_vec.shape)\n",
    "        print('news_vec', news_vec.shape)\n",
    "        return logits, user_vec, news_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "click torch.Size([2, 50, 30, 300])\n",
      "can torch.Size([2, 5, 30, 300])\n",
      "TimeDist_x torch.Size([2, 50, 30, 300])\n",
      "doc_encoder:phase1 torch.Size([2, 28, 400])\n",
      "doc_encoder:attention torch.Size([2, 28, 400])\n",
      "doc_encoder:phase2 torch.Size([2, 400])\n",
      "doc_encoder:phase1 torch.Size([2, 28, 400])\n",
      "doc_encoder:attention torch.Size([2, 28, 400])\n",
      "doc_encoder:phase2 torch.Size([2, 400])\n",
      "doc_encoder:phase1 torch.Size([2, 28, 400])\n",
      "doc_encoder:attention torch.Size([2, 28, 400])\n",
      "doc_encoder:phase2 torch.Size([2, 400])\n",
      "doc_encoder:phase1 torch.Size([2, 28, 400])\n",
      "doc_encoder:attention torch.Size([2, 28, 400])\n",
      "doc_encoder:phase2 torch.Size([2, 400])\n",
      "doc_encoder:phase1 torch.Size([2, 28, 400])\n",
      "doc_encoder:attention torch.Size([2, 28, 400])\n",
      "doc_encoder:phase2 torch.Size([2, 400])\n",
      "doc_encoder:phase1 torch.Size([2, 28, 400])\n",
      "doc_encoder:attention torch.Size([2, 28, 400])\n",
      "doc_encoder:phase2 torch.Size([2, 400])\n",
      "doc_encoder:phase1 torch.Size([2, 28, 400])\n",
      "doc_encoder:attention torch.Size([2, 28, 400])\n",
      "doc_encoder:phase2 torch.Size([2, 400])\n",
      "doc_encoder:phase1 torch.Size([2, 28, 400])\n",
      "doc_encoder:attention torch.Size([2, 28, 400])\n",
      "doc_encoder:phase2 torch.Size([2, 400])\n",
      "doc_encoder:phase1 torch.Size([2, 28, 400])\n",
      "doc_encoder:attention torch.Size([2, 28, 400])\n",
      "doc_encoder:phase2 torch.Size([2, 400])\n",
      "doc_encoder:phase1 torch.Size([2, 28, 400])\n",
      "doc_encoder:attention torch.Size([2, 28, 400])\n",
      "doc_encoder:phase2 torch.Size([2, 400])\n",
      "doc_encoder:phase1 torch.Size([2, 28, 400])\n",
      "doc_encoder:attention torch.Size([2, 28, 400])\n",
      "doc_encoder:phase2 torch.Size([2, 400])\n",
      "doc_encoder:phase1 torch.Size([2, 28, 400])\n",
      "doc_encoder:attention torch.Size([2, 28, 400])\n",
      "doc_encoder:phase2 torch.Size([2, 400])\n",
      "doc_encoder:phase1 torch.Size([2, 28, 400])\n",
      "doc_encoder:attention torch.Size([2, 28, 400])\n",
      "doc_encoder:phase2 torch.Size([2, 400])\n",
      "doc_encoder:phase1 torch.Size([2, 28, 400])\n",
      "doc_encoder:attention torch.Size([2, 28, 400])\n",
      "doc_encoder:phase2 torch.Size([2, 400])\n",
      "doc_encoder:phase1 torch.Size([2, 28, 400])\n",
      "doc_encoder:attention torch.Size([2, 28, 400])\n",
      "doc_encoder:phase2 torch.Size([2, 400])\n",
      "doc_encoder:phase1 torch.Size([2, 28, 400])\n",
      "doc_encoder:attention torch.Size([2, 28, 400])\n",
      "doc_encoder:phase2 torch.Size([2, 400])\n",
      "doc_encoder:phase1 torch.Size([2, 28, 400])\n",
      "doc_encoder:attention torch.Size([2, 28, 400])\n",
      "doc_encoder:phase2 torch.Size([2, 400])\n",
      "doc_encoder:phase1 torch.Size([2, 28, 400])\n",
      "doc_encoder:attention torch.Size([2, 28, 400])\n",
      "doc_encoder:phase2 torch.Size([2, 400])\n",
      "doc_encoder:phase1 torch.Size([2, 28, 400])\n",
      "doc_encoder:attention torch.Size([2, 28, 400])\n",
      "doc_encoder:phase2 torch.Size([2, 400])\n",
      "doc_encoder:phase1 torch.Size([2, 28, 400])\n",
      "doc_encoder:attention torch.Size([2, 28, 400])\n",
      "doc_encoder:phase2 torch.Size([2, 400])\n",
      "doc_encoder:phase1 torch.Size([2, 28, 400])\n",
      "doc_encoder:attention torch.Size([2, 28, 400])\n",
      "doc_encoder:phase2 torch.Size([2, 400])\n",
      "doc_encoder:phase1 torch.Size([2, 28, 400])\n",
      "doc_encoder:attention torch.Size([2, 28, 400])\n",
      "doc_encoder:phase2 torch.Size([2, 400])\n",
      "doc_encoder:phase1 torch.Size([2, 28, 400])\n",
      "doc_encoder:attention torch.Size([2, 28, 400])\n",
      "doc_encoder:phase2 torch.Size([2, 400])\n",
      "doc_encoder:phase1 torch.Size([2, 28, 400])\n",
      "doc_encoder:attention torch.Size([2, 28, 400])\n",
      "doc_encoder:phase2 torch.Size([2, 400])\n",
      "doc_encoder:phase1 torch.Size([2, 28, 400])\n",
      "doc_encoder:attention torch.Size([2, 28, 400])\n",
      "doc_encoder:phase2 torch.Size([2, 400])\n",
      "doc_encoder:phase1 torch.Size([2, 28, 400])\n",
      "doc_encoder:attention torch.Size([2, 28, 400])\n",
      "doc_encoder:phase2 torch.Size([2, 400])\n",
      "doc_encoder:phase1 torch.Size([2, 28, 400])\n",
      "doc_encoder:attention torch.Size([2, 28, 400])\n",
      "doc_encoder:phase2 torch.Size([2, 400])\n",
      "doc_encoder:phase1 torch.Size([2, 28, 400])\n",
      "doc_encoder:attention torch.Size([2, 28, 400])\n",
      "doc_encoder:phase2 torch.Size([2, 400])\n",
      "doc_encoder:phase1 torch.Size([2, 28, 400])\n",
      "doc_encoder:attention torch.Size([2, 28, 400])\n",
      "doc_encoder:phase2 torch.Size([2, 400])\n",
      "doc_encoder:phase1 torch.Size([2, 28, 400])\n",
      "doc_encoder:attention torch.Size([2, 28, 400])\n",
      "doc_encoder:phase2 torch.Size([2, 400])\n",
      "doc_encoder:phase1 torch.Size([2, 28, 400])\n",
      "doc_encoder:attention torch.Size([2, 28, 400])\n",
      "doc_encoder:phase2 torch.Size([2, 400])\n",
      "doc_encoder:phase1 torch.Size([2, 28, 400])\n",
      "doc_encoder:attention torch.Size([2, 28, 400])\n",
      "doc_encoder:phase2 torch.Size([2, 400])\n",
      "doc_encoder:phase1 torch.Size([2, 28, 400])\n",
      "doc_encoder:attention torch.Size([2, 28, 400])\n",
      "doc_encoder:phase2 torch.Size([2, 400])\n",
      "doc_encoder:phase1 torch.Size([2, 28, 400])\n",
      "doc_encoder:attention torch.Size([2, 28, 400])\n",
      "doc_encoder:phase2 torch.Size([2, 400])\n",
      "doc_encoder:phase1 torch.Size([2, 28, 400])\n",
      "doc_encoder:attention torch.Size([2, 28, 400])\n",
      "doc_encoder:phase2 torch.Size([2, 400])\n",
      "doc_encoder:phase1 torch.Size([2, 28, 400])\n",
      "doc_encoder:attention torch.Size([2, 28, 400])\n",
      "doc_encoder:phase2 torch.Size([2, 400])\n",
      "doc_encoder:phase1 torch.Size([2, 28, 400])\n",
      "doc_encoder:attention torch.Size([2, 28, 400])\n",
      "doc_encoder:phase2 torch.Size([2, 400])\n",
      "doc_encoder:phase1 torch.Size([2, 28, 400])\n",
      "doc_encoder:attention torch.Size([2, 28, 400])\n",
      "doc_encoder:phase2 torch.Size([2, 400])\n",
      "doc_encoder:phase1 torch.Size([2, 28, 400])\n",
      "doc_encoder:attention torch.Size([2, 28, 400])\n",
      "doc_encoder:phase2 torch.Size([2, 400])\n",
      "doc_encoder:phase1 torch.Size([2, 28, 400])\n",
      "doc_encoder:attention torch.Size([2, 28, 400])\n",
      "doc_encoder:phase2 torch.Size([2, 400])\n",
      "doc_encoder:phase1 torch.Size([2, 28, 400])\n",
      "doc_encoder:attention torch.Size([2, 28, 400])\n",
      "doc_encoder:phase2 torch.Size([2, 400])\n",
      "doc_encoder:phase1 torch.Size([2, 28, 400])\n",
      "doc_encoder:attention torch.Size([2, 28, 400])\n",
      "doc_encoder:phase2 torch.Size([2, 400])\n",
      "doc_encoder:phase1 torch.Size([2, 28, 400])\n",
      "doc_encoder:attention torch.Size([2, 28, 400])\n",
      "doc_encoder:phase2 torch.Size([2, 400])\n",
      "doc_encoder:phase1 torch.Size([2, 28, 400])\n",
      "doc_encoder:attention torch.Size([2, 28, 400])\n",
      "doc_encoder:phase2 torch.Size([2, 400])\n",
      "doc_encoder:phase1 torch.Size([2, 28, 400])\n",
      "doc_encoder:attention torch.Size([2, 28, 400])\n",
      "doc_encoder:phase2 torch.Size([2, 400])\n",
      "doc_encoder:phase1 torch.Size([2, 28, 400])\n",
      "doc_encoder:attention torch.Size([2, 28, 400])\n",
      "doc_encoder:phase2 torch.Size([2, 400])\n",
      "doc_encoder:phase1 torch.Size([2, 28, 400])\n",
      "doc_encoder:attention torch.Size([2, 28, 400])\n",
      "doc_encoder:phase2 torch.Size([2, 400])\n",
      "doc_encoder:phase1 torch.Size([2, 28, 400])\n",
      "doc_encoder:attention torch.Size([2, 28, 400])\n",
      "doc_encoder:phase2 torch.Size([2, 400])\n",
      "doc_encoder:phase1 torch.Size([2, 28, 400])\n",
      "doc_encoder:attention torch.Size([2, 28, 400])\n",
      "doc_encoder:phase2 torch.Size([2, 400])\n",
      "doc_encoder:phase1 torch.Size([2, 28, 400])\n",
      "doc_encoder:attention torch.Size([2, 28, 400])\n",
      "doc_encoder:phase2 torch.Size([2, 400])\n",
      "TimeDist_output torch.Size([2, 50, 400])\n",
      "click_vecs (None, 50, 400) torch.Size([2, 50, 400])\n",
      "TimeDist_x torch.Size([2, 5, 30, 300])\n",
      "doc_encoder:phase1 torch.Size([2, 28, 400])\n",
      "doc_encoder:attention torch.Size([2, 28, 400])\n",
      "doc_encoder:phase2 torch.Size([2, 400])\n",
      "doc_encoder:phase1 torch.Size([2, 28, 400])\n",
      "doc_encoder:attention torch.Size([2, 28, 400])\n",
      "doc_encoder:phase2 torch.Size([2, 400])\n",
      "doc_encoder:phase1 torch.Size([2, 28, 400])\n",
      "doc_encoder:attention torch.Size([2, 28, 400])\n",
      "doc_encoder:phase2 torch.Size([2, 400])\n",
      "doc_encoder:phase1 torch.Size([2, 28, 400])\n",
      "doc_encoder:attention torch.Size([2, 28, 400])\n",
      "doc_encoder:phase2 torch.Size([2, 400])\n",
      "doc_encoder:phase1 torch.Size([2, 28, 400])\n",
      "doc_encoder:attention torch.Size([2, 28, 400])\n",
      "doc_encoder:phase2 torch.Size([2, 400])\n",
      "TimeDist_output torch.Size([2, 5, 400])\n",
      "can_vecs (None, 5, 400) torch.Size([2, 5, 400])\n",
      "news_vecs_input torch.Size([2, 50, 400])\n",
      "pool2_user_vec2 torch.Size([2, 400])\n",
      "tail2_user_vecs1 torch.Size([2, 20, 400])\n",
      "gru2_user_vec1 torch.Size([2, 20, 400])\n",
      "torch.Size([2, 2, 400])\n",
      "torch.Size([2, 400])\n",
      "user_vec (None, 400) torch.Size([2, 400])\n",
      "scores  (None, 5) torch.Size([2, 5])\n",
      "logits  (None, 5) torch.Size([2, 5])\n",
      "doc_encoder:phase1 torch.Size([2, 28, 400])\n",
      "doc_encoder:attention torch.Size([2, 28, 400])\n",
      "doc_encoder:phase2 torch.Size([2, 400])\n",
      "user_vec torch.Size([2, 400])\n",
      "news_vec torch.Size([2, 400])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0.2006, 0.2008, 0.1978, 0.2008, 0.1999],\n",
       "         [0.2023, 0.2005, 0.1980, 0.2003, 0.1989]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[-6.3956e-03, -9.1935e-03, -4.7749e-03,  3.4982e-02,  1.1907e-02,\n",
       "          -5.6878e-02, -2.7117e-02, -4.9031e-04,  7.8465e-03, -8.6454e-03,\n",
       "          -2.5444e-02, -3.8356e-02,  3.7833e-03,  1.9825e-02,  0.0000e+00,\n",
       "          -5.1067e-02, -3.3640e-02,  4.5992e-02,  1.8659e-02,  5.5399e-03,\n",
       "          -2.7899e-02, -1.0118e-02,  7.9805e-03,  4.8648e-02,  4.8376e-02,\n",
       "           2.4903e-02, -3.3250e-02, -1.2652e-02,  7.0006e-03,  5.0891e-02,\n",
       "           3.6230e-02,  1.3786e-02, -6.1919e-02,  3.0208e-02,  5.6953e-04,\n",
       "          -2.4975e-03, -1.8701e-02,  1.7812e-03, -1.4314e-02,  1.0928e-02,\n",
       "           1.6616e-02, -1.1800e-02,  2.0700e-02,  6.1527e-03, -1.7211e-03,\n",
       "          -2.2236e-02, -5.0913e-02, -8.7408e-03,  5.7324e-02,  4.2661e-02,\n",
       "          -3.1426e-04, -2.3402e-02, -1.2742e-02,  3.3155e-02, -2.1730e-03,\n",
       "          -3.7434e-03,  2.2314e-02, -4.3151e-02,  1.1460e-02,  5.7077e-02,\n",
       "          -1.3392e-02, -2.9493e-03, -1.0098e-02, -4.9403e-02,  9.6381e-03,\n",
       "           6.9886e-04, -5.6650e-02,  3.3610e-02,  5.7373e-02, -4.0875e-03,\n",
       "           2.6808e-02, -6.3790e-02, -9.4184e-03,  6.6709e-02, -3.2271e-02,\n",
       "          -3.6678e-03,  8.8159e-03, -2.5892e-02,  2.8192e-02, -1.1735e-02,\n",
       "           8.6375e-02, -7.1983e-02, -2.6021e-02,  2.4120e-02,  5.3105e-02,\n",
       "           0.0000e+00, -1.0643e-03,  8.1247e-02, -1.8364e-02,  3.3164e-02,\n",
       "           1.1469e-02, -8.6545e-02, -6.7999e-02,  1.6694e-02, -2.1776e-02,\n",
       "           7.8190e-03, -6.7930e-02, -3.1964e-02, -7.9693e-02, -7.8900e-03,\n",
       "          -5.4906e-02,  1.0463e-02, -7.1716e-03,  4.2139e-02,  1.0041e-02,\n",
       "          -2.5659e-03,  2.1463e-02,  2.4886e-02,  2.9147e-02, -2.2974e-02,\n",
       "          -4.0680e-03, -1.3703e-02,  4.2872e-03, -4.0312e-03,  4.4209e-02,\n",
       "           6.0118e-02, -8.7188e-02,  5.3552e-02, -2.1635e-02,  1.8670e-02,\n",
       "          -3.2822e-02, -2.9268e-02, -2.4720e-02,  1.0602e-02,  3.0397e-03,\n",
       "          -1.0504e-02, -4.4517e-02,  3.2500e-02,  1.9048e-03,  2.5259e-02,\n",
       "          -1.0771e-02,  1.8269e-02, -8.8506e-03,  3.8750e-02, -5.1591e-02,\n",
       "           4.4649e-03,  1.1090e-02, -1.8374e-02, -1.5602e-03,  9.9351e-03,\n",
       "           0.0000e+00,  5.4935e-02,  1.7863e-02, -5.7412e-03,  1.5843e-02,\n",
       "          -3.0768e-02, -5.4058e-03,  1.9104e-02,  4.4067e-02,  0.0000e+00,\n",
       "           9.2186e-03, -2.9840e-02,  5.3540e-02,  3.9327e-02, -1.9981e-02,\n",
       "          -2.1889e-03, -1.6943e-02,  3.4944e-02, -3.0699e-02,  3.9410e-02,\n",
       "          -1.7654e-03, -1.0494e-03, -5.1577e-02, -1.8415e-02, -8.5156e-03,\n",
       "           7.0850e-02,  3.4837e-02, -4.6170e-03, -2.5445e-02,  3.5556e-02,\n",
       "          -6.7506e-03,  3.3267e-02, -2.7777e-02, -2.9574e-02,  4.3287e-02,\n",
       "           3.7628e-02, -4.6006e-02,  3.1168e-03, -1.0200e-03, -7.6130e-02,\n",
       "          -5.9399e-02,  2.3730e-02, -8.5561e-03, -4.4642e-02,  2.4398e-02,\n",
       "          -1.6023e-02,  7.4717e-02,  2.9592e-02,  7.4178e-02,  0.0000e+00,\n",
       "           2.3954e-02, -4.0617e-02, -5.4293e-02, -1.2306e-02,  3.3911e-02,\n",
       "           1.5525e-02,  3.3041e-02, -4.1377e-02, -1.4560e-02, -1.9903e-02,\n",
       "           4.2982e-02,  5.4369e-02,  5.6773e-03, -2.1306e-03, -1.7675e-02,\n",
       "          -2.1840e-03, -1.7659e-02,  1.3054e-02, -4.0862e-02,  3.7510e-02,\n",
       "           1.5832e-03, -1.2301e-02, -4.0070e-02,  1.1056e-02, -5.3905e-02,\n",
       "          -2.9582e-02,  4.3845e-02,  5.9394e-02, -3.0920e-02,  9.5534e-03,\n",
       "           1.9369e-02,  6.8393e-02, -3.5816e-02,  1.1690e-02,  6.3818e-02,\n",
       "          -5.4985e-03,  3.7166e-02, -5.9782e-02,  1.2947e-02,  6.7361e-02,\n",
       "           3.5238e-02, -1.7606e-03, -3.7723e-02, -2.9234e-02,  1.0334e-02,\n",
       "          -1.6409e-02, -1.0292e-02, -3.0612e-02, -1.3543e-02, -3.2968e-02,\n",
       "           1.4805e-02,  5.7939e-02, -3.6918e-02, -5.4316e-02, -2.3956e-03,\n",
       "          -1.6842e-02, -4.7202e-02, -3.1451e-02, -2.8005e-02,  2.5633e-02,\n",
       "           2.8186e-02, -6.5743e-02, -3.6299e-02,  1.5540e-02, -2.0062e-02,\n",
       "           2.9219e-03, -2.7791e-02,  3.3382e-02,  0.0000e+00,  2.2406e-02,\n",
       "           0.0000e+00,  3.7213e-02,  3.2479e-02,  1.2271e-02, -2.2096e-02,\n",
       "           4.2358e-03, -1.6079e-03, -5.2771e-02, -5.9267e-02,  2.1225e-02,\n",
       "          -1.4260e-02, -2.5521e-02, -3.9988e-03, -4.2540e-03, -7.2959e-03,\n",
       "          -3.0737e-02, -3.8316e-02,  1.9279e-02, -3.0688e-02, -1.9045e-02,\n",
       "           3.6359e-03, -2.9752e-03, -1.7783e-02,  2.8008e-02,  6.5971e-02,\n",
       "           9.5207e-03, -6.5627e-02, -2.3113e-02, -3.8135e-02,  1.4387e-03,\n",
       "           1.6585e-02, -4.2692e-02,  2.4102e-02, -5.3653e-03,  0.0000e+00,\n",
       "          -1.7469e-02,  2.5080e-03, -1.2656e-02, -1.8621e-02,  6.5299e-02,\n",
       "          -2.8049e-04, -2.9451e-02,  2.5978e-02,  2.2384e-02, -3.0836e-03,\n",
       "          -2.6927e-02,  5.3233e-02, -1.1364e-02,  1.5863e-02, -3.4498e-02,\n",
       "          -2.5022e-03, -3.1787e-02, -2.9867e-02, -3.2024e-02, -5.6077e-02,\n",
       "          -4.7908e-02,  2.7121e-02,  4.3559e-02,  1.2691e-02,  1.2338e-02,\n",
       "          -3.7956e-02,  3.5825e-03, -2.6999e-02,  2.6820e-02, -4.5245e-02,\n",
       "           5.2643e-02,  3.8762e-02, -2.5486e-02,  2.8603e-03,  0.0000e+00,\n",
       "           0.0000e+00,  1.2640e-02, -5.7583e-02, -8.1964e-02, -1.3995e-02,\n",
       "           4.3635e-02,  5.6681e-02, -1.8266e-02,  1.2055e-02, -8.2972e-03,\n",
       "           8.2592e-02, -1.5671e-02,  1.2431e-02,  7.4307e-03,  2.8239e-02,\n",
       "          -3.6760e-02,  1.5882e-02,  2.2770e-02, -2.5380e-02,  4.0135e-02,\n",
       "          -1.2202e-03, -2.1632e-02, -3.6674e-02,  6.0719e-03,  2.6040e-02,\n",
       "           6.6932e-03, -7.3454e-03,  1.1671e-02, -3.1850e-02, -3.6853e-02,\n",
       "          -1.7804e-02, -2.9696e-02,  2.0203e-02, -1.9780e-02,  1.4228e-02,\n",
       "           5.3540e-02, -1.5412e-02, -3.4983e-02,  2.6866e-02, -9.7358e-03,\n",
       "           2.3188e-02, -4.0479e-02,  8.5781e-03, -1.8332e-02,  3.3235e-02,\n",
       "          -2.5703e-02, -2.4042e-02,  3.6571e-02,  2.1599e-02,  4.0837e-02,\n",
       "           2.5147e-02,  5.9543e-02,  2.2970e-02, -2.1251e-02,  1.6784e-02,\n",
       "          -2.7388e-02, -4.7882e-02,  2.7808e-03,  5.2201e-02,  1.2658e-02,\n",
       "          -7.1739e-03,  6.8022e-03, -1.2447e-02,  2.3882e-05,  8.4704e-03,\n",
       "           0.0000e+00, -9.5171e-03,  1.5292e-02, -1.2650e-02, -1.1908e-02],\n",
       "         [-1.0321e-02, -1.8512e-02,  3.6196e-02,  8.9939e-03,  1.3530e-02,\n",
       "          -6.0589e-02, -2.2794e-02, -2.0532e-02, -9.1976e-03, -4.0383e-03,\n",
       "          -2.2319e-02,  0.0000e+00,  3.6966e-03, -3.3291e-03,  5.9563e-04,\n",
       "          -4.8701e-02, -2.1283e-02,  1.1075e-02,  7.9783e-03,  7.2547e-04,\n",
       "          -1.1588e-02, -5.2740e-03,  1.3668e-03, -1.2975e-02,  4.6977e-02,\n",
       "           1.3753e-02, -3.1187e-02, -1.3952e-02, -1.5762e-04,  4.6651e-02,\n",
       "          -4.7673e-03,  1.1997e-02, -6.5539e-02,  3.5009e-02, -1.6419e-02,\n",
       "          -6.4576e-03, -7.0356e-03,  4.4295e-03, -4.8435e-03,  1.3074e-02,\n",
       "           2.2330e-02, -4.0636e-03,  2.4764e-02,  2.6532e-02,  1.9068e-02,\n",
       "           5.6219e-03, -3.7945e-02, -9.6611e-03,  3.7901e-02,  4.7901e-02,\n",
       "           1.4730e-04, -2.3943e-02, -1.4567e-02,  3.8827e-02, -4.5255e-03,\n",
       "           1.1685e-03,  2.7906e-02, -4.2380e-02, -3.3916e-03,  5.4447e-02,\n",
       "          -1.2028e-02, -5.5041e-03, -2.8210e-02, -5.0477e-02, -2.9987e-02,\n",
       "          -7.8890e-03, -6.0650e-02,  3.3271e-02,  4.8528e-02,  2.6143e-02,\n",
       "           2.9811e-02, -6.4570e-02, -8.2206e-03,  7.0521e-02, -3.9596e-02,\n",
       "          -2.8208e-03,  5.0136e-03, -2.4741e-02,  4.8576e-02,  1.8499e-03,\n",
       "           5.4554e-02, -7.5728e-02, -2.7769e-02,  2.3497e-02,  3.7578e-02,\n",
       "          -6.6858e-02,  6.3417e-04,  6.4770e-02, -1.9362e-02,  1.2472e-02,\n",
       "           9.1048e-03, -8.6400e-02, -6.7482e-02,  2.2973e-02, -3.6891e-02,\n",
       "           1.0955e-02, -6.6242e-02, -1.3145e-02, -7.6661e-02, -1.6178e-02,\n",
       "          -5.0328e-02,  1.3624e-02, -1.0599e-02,  3.4902e-03,  5.1024e-03,\n",
       "           1.4024e-02,  1.9349e-02,  2.4781e-02,  0.0000e+00, -1.0799e-02,\n",
       "          -9.9276e-03, -1.2477e-02, -2.6904e-02,  2.1523e-02,  4.4418e-02,\n",
       "           4.8426e-02, -8.6236e-02, -6.9685e-03, -2.5637e-02,  2.6923e-02,\n",
       "          -3.5834e-02, -1.3842e-02,  1.9414e-02,  2.3747e-02,  1.3603e-02,\n",
       "          -1.0060e-02, -4.4106e-02,  3.3315e-02,  4.2950e-04,  2.7540e-02,\n",
       "          -9.2829e-03,  2.8239e-02, -1.4148e-02,  4.2024e-02, -5.3035e-02,\n",
       "           2.4508e-04, -1.2909e-02, -4.8382e-02, -7.4206e-02,  6.6774e-03,\n",
       "          -8.5147e-03,  4.0086e-02,  1.3514e-02, -9.5944e-03,  1.4330e-02,\n",
       "           6.9226e-03, -1.5769e-03,  1.9514e-02, -1.3044e-02,  4.0056e-02,\n",
       "           1.2865e-02, -2.8672e-02,  5.4541e-02,  0.0000e+00, -9.6885e-04,\n",
       "           1.3432e-03, -1.5259e-03,  3.0690e-02, -1.5041e-02,  3.6026e-02,\n",
       "          -1.7380e-03, -9.6681e-04, -4.2717e-02, -1.5653e-02, -1.5045e-02,\n",
       "           7.4586e-02,  1.1122e-02, -2.9716e-02,  5.0872e-03,  3.3682e-02,\n",
       "          -1.3989e-02,  1.0477e-02, -4.6858e-03, -2.9112e-02,  7.0829e-02,\n",
       "           3.9014e-02, -4.7203e-02,  9.3836e-04,  3.8080e-03, -7.4188e-02,\n",
       "          -5.0691e-02, -2.4772e-03, -1.5913e-02, -6.6873e-02,  2.2501e-02,\n",
       "          -2.8915e-02, -1.8327e-02,  0.0000e+00,  6.8832e-02,  3.2082e-02,\n",
       "           6.3536e-03, -3.8205e-02,  0.0000e+00,  2.6921e-03, -2.6226e-02,\n",
       "          -3.2974e-03,  3.2650e-02, -8.7281e-03,  4.8883e-03,  5.3352e-03,\n",
       "           1.1407e-02,  6.2085e-02, -4.6378e-02,  3.3729e-02,  3.4132e-02,\n",
       "           4.7220e-03, -9.0278e-03,  1.0994e-02, -1.7954e-02,  3.2585e-02,\n",
       "          -2.3144e-02, -1.5575e-02, -4.2992e-02,  5.2642e-02, -6.1071e-02,\n",
       "          -2.6454e-02,  2.9531e-02,  3.2217e-02, -2.3817e-02,  1.6285e-02,\n",
       "           2.4763e-02,  6.7862e-02, -3.1896e-02,  5.4229e-02,  7.2538e-02,\n",
       "          -8.5247e-03,  3.7110e-02,  6.4020e-04,  1.2765e-02,  6.6669e-02,\n",
       "           3.1644e-02, -5.3175e-03, -3.1242e-02, -2.7055e-02, -3.5670e-02,\n",
       "          -1.7399e-02, -8.1072e-03, -1.9490e-02,  1.2715e-02, -3.4366e-02,\n",
       "           1.9079e-02, -3.2934e-02, -3.5227e-02, -1.7014e-02, -2.1098e-02,\n",
       "          -1.7617e-02, -4.6020e-02, -2.9130e-02, -3.1444e-02,  3.4742e-02,\n",
       "           2.8019e-02, -5.7835e-02, -3.9721e-02,  0.0000e+00,  4.7472e-03,\n",
       "           1.5199e-02, -2.8184e-02,  1.4996e-02, -4.7573e-02,  2.0768e-02,\n",
       "           8.6824e-03,  4.9699e-04,  3.4753e-02,  1.0953e-02, -1.8687e-02,\n",
       "           7.6059e-03, -1.9967e-02, -3.8834e-02, -5.4765e-02,  2.3647e-02,\n",
       "          -1.1077e-02,  0.0000e+00,  0.0000e+00, -1.8834e-04, -7.9593e-03,\n",
       "          -3.4506e-02, -1.2441e-02,  2.2557e-02, -2.8047e-02, -3.9895e-02,\n",
       "           1.4644e-03, -3.2586e-03,  2.0045e-02,  2.4890e-02,  0.0000e+00,\n",
       "           1.0573e-02, -5.0898e-02, -1.9794e-02, -4.0505e-02, -2.3843e-02,\n",
       "           1.8784e-02, -4.6121e-02,  2.1317e-02, -7.7367e-04, -1.5067e-02,\n",
       "          -6.3371e-03,  2.1997e-03, -1.3470e-02, -9.8432e-02,  5.9620e-02,\n",
       "          -2.8140e-02, -1.6794e-02,  3.5399e-02,  1.8239e-02,  7.8165e-02,\n",
       "           7.0224e-03,  5.2966e-02,  4.7200e-02, -6.8217e-02, -2.3463e-02,\n",
       "          -2.4606e-02, -2.9284e-02, -4.5198e-02, -3.6277e-02,  2.0458e-02,\n",
       "          -3.2220e-02,  3.1530e-02,  4.7350e-02,  1.0822e-02,  1.4847e-02,\n",
       "          -3.6864e-02,  5.5827e-03, -2.4188e-02, -8.8566e-03, -4.3662e-02,\n",
       "           2.5174e-02, -2.2721e-02,  5.0873e-03, -1.0248e-02,  6.2802e-03,\n",
       "          -8.3049e-03,  5.1557e-02, -5.6626e-02,  0.0000e+00, -1.9224e-02,\n",
       "           4.5153e-02,  4.4141e-02, -2.3944e-02,  9.1375e-03, -1.4355e-02,\n",
       "           5.6130e-02, -1.5232e-02,  1.0344e-02,  4.7224e-03,  1.1975e-02,\n",
       "          -3.4384e-02,  3.0619e-02, -2.3380e-02, -3.2623e-02,  1.0807e-02,\n",
       "           8.4663e-03,  0.0000e+00, -7.3095e-03,  4.2288e-02,  2.6886e-02,\n",
       "           1.2146e-02, -1.7699e-02,  1.0539e-02, -2.6526e-02,  0.0000e+00,\n",
       "          -1.8594e-02, -2.6995e-02, -3.3150e-02,  7.5327e-03, -1.2574e-02,\n",
       "           4.1987e-02, -1.6648e-02, -3.6289e-02,  5.7283e-02, -5.6883e-03,\n",
       "           4.1179e-02, -3.0424e-02,  8.1945e-04,  1.5066e-02,  3.3383e-02,\n",
       "          -3.1463e-02, -2.0266e-02,  4.0017e-02,  1.9561e-02,  4.1987e-02,\n",
       "           1.9729e-02,  5.8160e-02,  4.5335e-02, -5.1251e-03,  1.9272e-02,\n",
       "          -2.4370e-02, -6.8149e-02,  4.5582e-03,  5.1138e-02,  5.9658e-03,\n",
       "          -7.7740e-03, -1.6741e-02, -9.7017e-03, -1.4915e-03,  5.7871e-03,\n",
       "          -4.6198e-02, -2.0841e-02,  0.0000e+00, -9.9353e-03, -1.2563e-02]],\n",
       "        grad_fn=<ViewBackward0>),\n",
       " tensor([[0.0000e+00, 9.6103e-02, 0.0000e+00, 9.6899e-02, 3.0868e-02, 0.0000e+00,\n",
       "          4.1473e-03, 4.7988e-02, 3.3142e-02, 0.0000e+00, 3.6845e-02, 1.4358e-03,\n",
       "          1.2143e-01, 0.0000e+00, 5.1636e-03, 0.0000e+00, 7.5442e-02, 1.6460e-01,\n",
       "          1.7250e-01, 1.4078e-01, 9.0739e-04, 1.5233e-02, 0.0000e+00, 0.0000e+00,\n",
       "          3.0672e-03, 1.9237e-02, 1.9334e-02, 5.8571e-02, 3.0835e-02, 0.0000e+00,\n",
       "          4.4885e-02, 2.6400e-02, 3.8757e-02, 3.2396e-02, 7.3315e-03, 4.0203e-02,\n",
       "          3.6915e-02, 1.5712e-02, 5.6925e-03, 2.7301e-02, 4.1547e-02, 2.6603e-02,\n",
       "          0.0000e+00, 8.6283e-02, 0.0000e+00, 2.6569e-02, 2.8266e-03, 1.2368e-02,\n",
       "          9.8223e-02, 2.7430e-03, 6.7312e-02, 0.0000e+00, 0.0000e+00, 1.2728e-01,\n",
       "          1.6924e-02, 3.4933e-02, 3.6020e-02, 0.0000e+00, 1.8117e-02, 3.0518e-03,\n",
       "          3.5295e-02, 4.0395e-02, 0.0000e+00, 2.1090e-02, 1.1189e-01, 5.3225e-02,\n",
       "          3.3304e-02, 5.6472e-02, 5.3407e-03, 1.5553e-02, 1.8829e-02, 3.5570e-02,\n",
       "          4.3296e-03, 5.7468e-02, 6.4961e-02, 9.8216e-02, 1.9485e-02, 7.5887e-04,\n",
       "          2.9820e-02, 0.0000e+00, 0.0000e+00, 7.3667e-02, 0.0000e+00, 6.3837e-02,\n",
       "          1.9039e-02, 2.0098e-02, 0.0000e+00, 0.0000e+00, 3.2355e-02, 1.2732e-01,\n",
       "          5.1074e-02, 1.4182e-01, 6.9550e-02, 3.1938e-02, 3.5824e-02, 3.1558e-02,\n",
       "          4.2956e-02, 1.5585e-01, 1.5929e-02, 2.8566e-03, 3.0123e-02, 2.6280e-02,\n",
       "          2.2336e-01, 0.0000e+00, 4.4289e-03, 2.9296e-02, 0.0000e+00, 4.0184e-02,\n",
       "          9.1750e-02, 5.3630e-02, 1.4589e-02, 0.0000e+00, 0.0000e+00, 1.4942e-01,\n",
       "          8.0724e-05, 3.9862e-02, 6.6890e-02, 7.9449e-02, 2.5798e-02, 0.0000e+00,\n",
       "          0.0000e+00, 1.1399e-03, 9.7979e-03, 7.2037e-03, 5.7665e-02, 0.0000e+00,\n",
       "          8.3421e-02, 1.7420e-02, 8.8204e-03, 8.2179e-03, 9.8252e-02, 1.2071e-01,\n",
       "          9.5466e-02, 7.0472e-02, 9.1364e-02, 1.1406e-02, 1.3791e-02, 8.6340e-02,\n",
       "          1.4934e-01, 4.7174e-02, 0.0000e+00, 4.0183e-02, 7.3460e-02, 8.7409e-03,\n",
       "          3.9782e-02, 3.7573e-02, 0.0000e+00, 1.4958e-01, 0.0000e+00, 0.0000e+00,\n",
       "          6.7264e-04, 8.6201e-03, 3.0958e-02, 6.8466e-04, 1.0897e-01, 6.0989e-02,\n",
       "          8.8623e-02, 2.0975e-03, 0.0000e+00, 9.8116e-03, 1.4853e-02, 5.0779e-02,\n",
       "          9.7746e-02, 5.4784e-02, 9.6998e-02, 5.5029e-04, 0.0000e+00, 1.6093e-02,\n",
       "          1.8094e-02, 7.7899e-03, 1.7100e-02, 5.0095e-03, 1.5627e-02, 8.6468e-02,\n",
       "          0.0000e+00, 4.6255e-02, 1.0018e-01, 1.0589e-01, 1.1511e-02, 6.2430e-04,\n",
       "          3.2183e-02, 6.0580e-03, 1.2147e-02, 1.1963e-02, 1.3096e-02, 4.3123e-03,\n",
       "          3.2466e-02, 6.5606e-02, 4.5662e-02, 1.0805e-02, 7.7958e-04, 1.6732e-01,\n",
       "          1.2867e-01, 1.7741e-01, 4.8551e-02, 3.8412e-02, 0.0000e+00, 0.0000e+00,\n",
       "          1.9546e-02, 1.1253e-03, 5.1088e-02, 6.2188e-03, 0.0000e+00, 5.9616e-02,\n",
       "          1.2304e-01, 8.4889e-02, 2.4648e-02, 8.3291e-02, 6.4826e-02, 1.7481e-02,\n",
       "          8.7418e-02, 5.3275e-02, 4.4432e-03, 1.0979e-01, 8.1398e-03, 1.7475e-02,\n",
       "          1.3514e-02, 1.1185e-02, 0.0000e+00, 1.4686e-03, 1.4565e-01, 4.6441e-02,\n",
       "          1.0191e-01, 6.1200e-02, 4.6773e-02, 1.5055e-03, 2.1783e-02, 1.7710e-01,\n",
       "          1.2038e-01, 0.0000e+00, 6.7040e-02, 1.3385e-03, 2.6963e-02, 1.6982e-03,\n",
       "          1.5044e-03, 7.8164e-02, 1.5517e-02, 6.1002e-02, 1.6127e-01, 0.0000e+00,\n",
       "          1.7035e-02, 1.4048e-01, 1.4565e-01, 1.3578e-01, 1.2136e-01, 2.4976e-03,\n",
       "          1.7384e-03, 4.8261e-03, 1.0630e-01, 5.3341e-03, 2.2916e-02, 0.0000e+00,\n",
       "          1.6543e-02, 1.0123e-01, 9.8879e-04, 5.8948e-02, 1.0605e-02, 5.1189e-02,\n",
       "          5.5509e-02, 5.0918e-02, 3.7814e-04, 2.8356e-02, 5.3811e-03, 1.3204e-03,\n",
       "          4.1964e-02, 1.3075e-01, 2.1473e-02, 6.4364e-02, 7.4168e-02, 1.4302e-01,\n",
       "          1.4718e-01, 0.0000e+00, 1.0088e-02, 1.1687e-02, 0.0000e+00, 4.2334e-02,\n",
       "          1.3528e-01, 1.4402e-01, 8.3370e-02, 0.0000e+00, 8.1236e-03, 7.4302e-02,\n",
       "          9.1542e-02, 5.5669e-02, 6.6314e-02, 4.5114e-02, 0.0000e+00, 1.9704e-01,\n",
       "          3.0518e-02, 4.7010e-02, 1.5084e-02, 0.0000e+00, 0.0000e+00, 6.7160e-03,\n",
       "          2.9379e-02, 2.0046e-02, 2.5061e-02, 1.3557e-01, 2.4177e-02, 1.6271e-01,\n",
       "          7.0168e-05, 2.0179e-01, 2.1443e-02, 7.8301e-02, 0.0000e+00, 5.6394e-02,\n",
       "          4.7525e-02, 5.7971e-02, 0.0000e+00, 1.4159e-02, 1.1525e-02, 3.4511e-02,\n",
       "          9.0503e-02, 3.3860e-03, 3.3073e-02, 3.0608e-02, 3.1204e-02, 1.7187e-02,\n",
       "          3.5230e-03, 1.3726e-02, 3.2323e-03, 7.9481e-02, 0.0000e+00, 7.4750e-02,\n",
       "          2.0038e-02, 3.3570e-02, 1.1812e-01, 6.0041e-02, 5.7501e-02, 6.3329e-02,\n",
       "          5.2441e-02, 6.0403e-02, 1.3824e-01, 4.6636e-02, 1.0156e-01, 2.0824e-01,\n",
       "          3.0450e-02, 8.9425e-02, 1.1171e-01, 1.0423e-03, 9.6721e-02, 1.0490e-02,\n",
       "          1.6262e-01, 2.2066e-03, 0.0000e+00, 9.4156e-03, 1.3023e-01, 6.0836e-03,\n",
       "          0.0000e+00, 1.9994e-02, 0.0000e+00, 1.8552e-02, 7.7416e-02, 1.8321e-02,\n",
       "          6.4877e-02, 1.5586e-02, 0.0000e+00, 0.0000e+00, 3.7111e-02, 3.5408e-02,\n",
       "          9.5030e-03, 1.5001e-04, 2.2615e-02, 2.6086e-02, 0.0000e+00, 9.2912e-02,\n",
       "          7.5134e-03, 4.3671e-02, 3.7531e-03, 5.3701e-02, 9.9457e-04, 7.7878e-04,\n",
       "          4.9147e-02, 9.5220e-03, 0.0000e+00, 3.0047e-03, 8.9858e-02, 5.0800e-02,\n",
       "          2.6716e-02, 0.0000e+00, 4.3069e-02, 9.7054e-03, 7.8349e-04, 5.7249e-02,\n",
       "          0.0000e+00, 2.7059e-02, 3.3121e-02, 1.8928e-02, 0.0000e+00, 3.1688e-02,\n",
       "          1.2721e-01, 8.9905e-03, 4.0199e-02, 2.1370e-03, 1.1355e-01, 9.9949e-02,\n",
       "          3.2136e-02, 8.2795e-02, 2.0082e-02, 2.5861e-01],\n",
       "         [0.0000e+00, 1.5114e-01, 0.0000e+00, 1.0335e-01, 2.4331e-02, 0.0000e+00,\n",
       "          1.8582e-03, 4.6550e-02, 4.1181e-02, 0.0000e+00, 2.2634e-02, 0.0000e+00,\n",
       "          1.1802e-01, 0.0000e+00, 2.4982e-03, 0.0000e+00, 6.9084e-02, 1.4573e-01,\n",
       "          2.5468e-01, 1.6212e-01, 7.2824e-04, 9.1723e-03, 2.3903e-03, 0.0000e+00,\n",
       "          3.2898e-03, 1.9450e-02, 1.7685e-02, 6.4569e-02, 3.1269e-02, 0.0000e+00,\n",
       "          7.3299e-02, 2.1889e-02, 4.5295e-02, 3.8675e-02, 1.2047e-02, 5.4859e-02,\n",
       "          3.8645e-02, 1.2125e-02, 1.1043e-02, 1.1505e-02, 5.3067e-02, 2.1498e-02,\n",
       "          0.0000e+00, 1.3085e-01, 0.0000e+00, 1.4275e-02, 2.7663e-03, 6.0787e-03,\n",
       "          1.2906e-01, 2.5875e-03, 5.5025e-02, 0.0000e+00, 0.0000e+00, 1.2182e-01,\n",
       "          1.7139e-02, 2.9634e-02, 3.8134e-02, 0.0000e+00, 1.4666e-02, 8.0407e-04,\n",
       "          2.9883e-02, 3.8230e-02, 0.0000e+00, 1.8084e-02, 9.6287e-02, 5.6133e-02,\n",
       "          3.5411e-02, 4.2087e-02, 2.7959e-03, 1.2714e-02, 1.3581e-02, 3.9692e-02,\n",
       "          5.3736e-03, 4.9047e-02, 5.0987e-02, 9.6120e-02, 1.7698e-02, 0.0000e+00,\n",
       "          2.1273e-02, 0.0000e+00, 0.0000e+00, 6.4887e-02, 1.2104e-03, 9.7068e-02,\n",
       "          1.5618e-02, 3.6015e-02, 0.0000e+00, 6.6549e-05, 3.1493e-02, 1.9910e-01,\n",
       "          7.1872e-02, 1.0626e-01, 6.3758e-02, 3.8482e-02, 3.4960e-02, 2.0847e-02,\n",
       "          6.8030e-02, 1.5477e-01, 7.2505e-03, 2.7649e-03, 4.3252e-02, 3.1518e-02,\n",
       "          2.0013e-01, 0.0000e+00, 4.4185e-03, 3.4604e-02, 0.0000e+00, 3.3174e-02,\n",
       "          1.2630e-01, 5.8307e-02, 2.0173e-02, 0.0000e+00, 0.0000e+00, 1.6946e-01,\n",
       "          1.3395e-04, 2.6740e-02, 1.0344e-01, 1.0730e-01, 3.1955e-02, 0.0000e+00,\n",
       "          0.0000e+00, 1.6896e-03, 8.9028e-03, 4.9482e-03, 4.8347e-02, 0.0000e+00,\n",
       "          8.5766e-02, 1.8055e-02, 8.0259e-03, 7.6427e-03, 6.8538e-02, 1.0663e-01,\n",
       "          8.8955e-02, 7.9888e-02, 8.8826e-02, 1.4170e-02, 1.5240e-02, 8.1560e-02,\n",
       "          1.3535e-01, 6.0646e-02, 0.0000e+00, 5.1450e-02, 6.4907e-02, 1.0265e-02,\n",
       "          5.4078e-02, 3.3068e-02, 0.0000e+00, 2.1266e-01, 0.0000e+00, 0.0000e+00,\n",
       "          8.7477e-04, 8.8082e-03, 2.1023e-02, 7.2309e-04, 1.1677e-01, 6.6521e-02,\n",
       "          8.6435e-02, 9.2033e-03, 0.0000e+00, 1.3569e-02, 1.4940e-02, 6.2028e-02,\n",
       "          8.6002e-02, 1.0096e-01, 6.7044e-02, 6.8340e-04, 0.0000e+00, 1.4211e-02,\n",
       "          8.9998e-03, 4.9233e-03, 1.2433e-02, 4.0095e-03, 1.2179e-02, 7.3952e-02,\n",
       "          0.0000e+00, 4.3327e-02, 9.2201e-02, 7.5829e-02, 8.1928e-03, 0.0000e+00,\n",
       "          3.5948e-02, 9.7479e-03, 5.0573e-03, 1.8166e-02, 1.4743e-02, 5.3842e-06,\n",
       "          4.2188e-02, 7.9010e-02, 2.6677e-02, 7.6639e-03, 9.8781e-04, 1.3703e-01,\n",
       "          1.5271e-01, 1.1349e-01, 7.1839e-02, 4.4996e-02, 1.5089e-03, 1.1158e-03,\n",
       "          1.7488e-02, 1.1064e-04, 3.5572e-02, 8.2500e-03, 0.0000e+00, 6.8668e-02,\n",
       "          1.2377e-01, 6.5815e-02, 2.3728e-02, 7.0323e-02, 5.5544e-02, 2.1999e-02,\n",
       "          9.7635e-02, 7.4581e-02, 2.4330e-03, 1.1013e-01, 6.7130e-03, 1.5155e-02,\n",
       "          7.4086e-03, 1.2906e-02, 0.0000e+00, 1.7808e-03, 1.0226e-01, 4.5286e-02,\n",
       "          7.3105e-02, 6.7289e-02, 5.3574e-02, 4.8348e-03, 2.5311e-02, 1.8312e-01,\n",
       "          1.1634e-01, 0.0000e+00, 6.2788e-02, 1.9646e-03, 1.3301e-02, 3.8359e-03,\n",
       "          1.4189e-03, 6.3156e-02, 1.6943e-02, 5.7935e-02, 1.6585e-01, 0.0000e+00,\n",
       "          1.6917e-02, 1.0997e-01, 1.2506e-01, 1.4393e-01, 1.3993e-01, 2.4920e-03,\n",
       "          1.8490e-03, 8.3204e-03, 5.6465e-02, 5.5562e-03, 4.0656e-02, 0.0000e+00,\n",
       "          1.0665e-02, 8.4143e-02, 3.1931e-04, 6.9266e-02, 9.6254e-03, 4.1282e-02,\n",
       "          7.3139e-02, 4.4201e-02, 0.0000e+00, 3.4610e-02, 5.4837e-03, 0.0000e+00,\n",
       "          3.3805e-02, 1.2204e-01, 1.7766e-02, 4.8121e-02, 5.9769e-02, 1.6176e-01,\n",
       "          1.6558e-01, 1.6162e-03, 1.7171e-02, 1.1763e-02, 0.0000e+00, 4.3583e-02,\n",
       "          1.3906e-01, 1.4327e-01, 4.3794e-02, 0.0000e+00, 1.2134e-03, 8.0490e-02,\n",
       "          7.7589e-02, 6.4907e-02, 3.6968e-02, 4.1195e-02, 0.0000e+00, 1.7878e-01,\n",
       "          2.6403e-02, 7.4885e-02, 4.4860e-03, 0.0000e+00, 1.2384e-03, 5.5618e-03,\n",
       "          2.4242e-02, 2.5042e-02, 1.7631e-02, 1.6650e-01, 3.6598e-02, 1.4299e-01,\n",
       "          1.0772e-05, 1.2617e-01, 1.4693e-02, 6.9871e-02, 0.0000e+00, 7.5571e-02,\n",
       "          4.3309e-02, 6.3882e-02, 0.0000e+00, 8.3825e-03, 1.4107e-02, 2.4653e-02,\n",
       "          7.3947e-02, 3.0739e-03, 2.2037e-02, 4.6732e-02, 4.1846e-02, 1.1609e-02,\n",
       "          7.4288e-03, 1.1577e-02, 1.8185e-03, 7.2072e-02, 0.0000e+00, 5.8838e-02,\n",
       "          3.3958e-02, 3.2736e-02, 9.4473e-02, 6.2339e-02, 5.2334e-02, 7.6384e-02,\n",
       "          6.7167e-02, 5.7382e-02, 1.6889e-01, 3.0078e-02, 1.2335e-01, 2.4833e-01,\n",
       "          3.2656e-02, 9.4271e-02, 1.0262e-01, 1.1233e-03, 1.1716e-01, 1.8343e-02,\n",
       "          2.0971e-01, 5.2361e-03, 0.0000e+00, 1.2292e-02, 1.2918e-01, 7.8921e-03,\n",
       "          0.0000e+00, 2.0605e-02, 1.5021e-03, 2.4535e-02, 7.2610e-02, 2.8317e-02,\n",
       "          8.1389e-02, 1.4345e-02, 0.0000e+00, 0.0000e+00, 5.4748e-02, 2.6086e-02,\n",
       "          5.5549e-03, 0.0000e+00, 2.2357e-02, 3.7325e-02, 0.0000e+00, 6.6869e-02,\n",
       "          4.3547e-03, 4.3389e-02, 8.7792e-04, 5.8279e-02, 3.0042e-04, 6.8901e-05,\n",
       "          5.1495e-02, 2.8293e-04, 0.0000e+00, 3.8730e-03, 1.1356e-01, 3.6589e-02,\n",
       "          4.9031e-02, 0.0000e+00, 3.3018e-02, 5.5458e-03, 0.0000e+00, 6.5271e-02,\n",
       "          0.0000e+00, 2.4190e-02, 4.1463e-02, 1.2655e-02, 0.0000e+00, 2.8779e-02,\n",
       "          1.2360e-01, 6.3640e-03, 2.1264e-02, 7.7753e-04, 1.1002e-01, 1.3022e-01,\n",
       "          4.0640e-02, 6.4898e-02, 2.2800e-02, 2.8307e-01]],\n",
       "        grad_fn=<ViewBackward0>))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bad pipe message: %s [b'\\x1f\\xe67lQ\\xa6\\xde\\x1f\\xec\\x1b0\\x07\\xedw\\xf2\\xf1`\\x99 \"\\x0c?d4w\\x1aN\\xde#\\x01\\x16\\x91\\xd3\\tv\\x8c\\xb2\\xad!\\xb6\\x9d\\xd9\\xf0\\x1e\\xae\\xfa\\xcb4\\xbd\\x89m\\x00\\x08\\x13\\x02\\x13\\x03\\x13\\x01\\x00\\xff\\x01\\x00\\x00\\x8f\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00', b'']\n",
      "Bad pipe message: %s [b':\\xdf\\xf5\\xe8\\x1fAi\\x0e\\n\\xe1P\\x99\\xca9^>3[ \\xabw\\xccD\\xd7\\xfc\\xc3H\\xb8{M\\x14\\xfe\\xa3ul\\x95\\nv\\x81\\xd5\\xce\\x02\\x12,M]\\xafT\\x9e\\xde\\xe4\\x00\\x08\\x13\\x02\\x13\\x03\\x13\\x01\\x00\\xff\\x01\\x00\\x00\\x8f\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17']\n",
      "Bad pipe message: %s [b'c\\xa8\\xd0l\\x83\\xb5\\x01\\x88\\x1b\\xab]\\xeb\\x7f\\xcd\\x9f\\x7f(\\xed\\x00\\x00|\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(']\n",
      "Bad pipe message: %s [b'^\\x18', b\"\\xc7\\xfd\\x8a\\xf8W\\x80\\xe5\\xf32\\x95\\xd6\\xb2\\x19*\\x00\\x00\\xa6\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0s\\xc0w\\x00\\xc4\\x00\\xc3\\xc0#\\xc0'\\x00g\\x00@\\xc0r\\xc0v\\x00\\xbe\\x00\\xbd\\xc0\\n\\xc0\\x14\\x009\\x008\\x00\\x88\\x00\\x87\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9a\\x00\\x99\\x00E\\x00D\\xc0\\x07\\xc0\\x11\\xc0\\x08\\xc0\\x12\\x00\\x16\\x00\\x13\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\"]\n",
      "Bad pipe message: %s [b\";t4\\xef'\\x9c\\x1a\\xdf\\x849\\xffD\\xd3&\\xda\\x97\\xa8~\\x00\\x00>\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\xc0\\x0f\\xc0\\x05\\x005\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\xc0\\x0e\\xc0\\x04\\x00\"]\n",
      "Bad pipe message: %s [b'\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01\\x00\\x00C\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0']\n",
      "Bad pipe message: %s [b'M\\xdc/X\\x7f\\xd3\\x06ad\\xfav\\x0c!9\\xbe\\xd8-4\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00']\n",
      "Bad pipe message: %s [b'\\x04\\xc0\\x12\\xc0']\n",
      "Bad pipe message: %s [b'\\x16\\x00\\x13\\x00\\x10\\x00\\r']\n",
      "Bad pipe message: %s [b'\\xfb\\xd9sydixq=Q\\xad\\xb79A\\xd0R9_\\x00\\x00>\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\xc0\\x0f\\xc0\\x05\\x005\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00', b'\\x11\\xc0\\x07\\xc0\\x0c\\xc0']\n",
      "Bad pipe message: %s [b'\\x05']\n",
      "Bad pipe message: %s [b'\\x1f6\\xd0\\x98p\\xe7\\xfa[\\xe3\\x1e>9\\x95\\xa3\\xf9\\x1cKH\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002']\n",
      "Bad pipe message: %s [b\"\\xd58\\x8a]\\xe6\\x8az\\x10\\x05\\xber\\xb4?\\x0f\\xdd\\xae/{\\x00\\x00\\x86\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\\xc01\\xc0-\\xc0)\\xc0%\\xc0\\x0e\\xc0\\x04\\x00\\x9c\\x00<\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\"]\n",
      "Bad pipe message: %s [b'\\xf3\\x1d\\x89!*h\\x7f+\\xbd\\n\\x19\\xf5y\\xd1-k\\x99', b'\\x00\\xf4\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5']\n"
     ]
    }
   ],
   "source": [
    "dim1 = 50\n",
    "dim2 = 400\n",
    "click = torch.randint(10, (2,50,30))\n",
    "can = torch.randint(10, (2,5,30))\n",
    "news = torch.randint(10, (2,30))\n",
    "\n",
    "x = torch.randn((2, dim1, dim2))\n",
    "\n",
    "f = FedNewsRec(mtx)\n",
    "#print(f.click_td)\n",
    "f.forward(click,can,news)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "542f8443ae01e5e89658f1cc6305b82f52ba1db662565f568700972549f4b0db"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('fedrec')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
